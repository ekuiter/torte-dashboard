{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# pip install plotly pandas statsmodels kaleido scipy nbformat jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import json\n",
    "import pickle\n",
    "import scipy\n",
    "from statistics import mean, stdev\n",
    "from math import sqrt, log10\n",
    "from packaging.version import Version\n",
    "\n",
    "init_json_path = \"vuetify-project/public/init.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(\n",
    "    stage, dtype={}, usecols=None, file=None, output_directory=\"output-linux\"\n",
    "):\n",
    "    if not file:\n",
    "        file = \"output\"\n",
    "    df = pd.read_csv(\n",
    "        f\"{output_directory}/{stage}/{file}.csv\", dtype=dtype, usecols=usecols\n",
    "    )\n",
    "    if \"committer_date_unix\" in df:\n",
    "        df[\"committer_date\"] = df[\"committer_date_unix\"].apply(\n",
    "            lambda d: pd.to_datetime(d, unit=\"s\")\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for drawing plots\n",
    "\n",
    "\n",
    "def estimate_group(group):\n",
    "    print(\"\\\\hspace{2mm} \" + group + \" \\\\\\\\\")\n",
    "\n",
    "\n",
    "def estimate_trend(\n",
    "    fig, color=None, color_value=None, xs=[], key=lambda x: x.timestamp()\n",
    "):\n",
    "    results = px.get_trendline_results(fig)\n",
    "    if color is not None and color_value is not None:\n",
    "        idx = [i for i, r in enumerate(results.iloc) if r[color] == color_value]\n",
    "        if idx != []:\n",
    "            idx = idx[0]\n",
    "        else:\n",
    "            idx = 0\n",
    "    else:\n",
    "        idx = 0\n",
    "    intercept = results.iloc[idx][\"px_fit_results\"].params[0]\n",
    "    slope = results.iloc[idx][\"px_fit_results\"].params[1]\n",
    "    daily = slope * pd.to_timedelta(1, unit=\"D\").total_seconds()\n",
    "    weekly = slope * pd.to_timedelta(7, unit=\"D\").total_seconds()\n",
    "    monthly = slope * pd.to_timedelta(1, unit=\"D\").total_seconds() * 30.437\n",
    "    yearly = slope * pd.to_timedelta(1, unit=\"D\").total_seconds() * 365.25\n",
    "    return daily, weekly, monthly, yearly, [intercept + slope * key(x) for x in xs]\n",
    "\n",
    "\n",
    "def log10_y_axis(fig):\n",
    "    fig.update_yaxes(tickprefix=\"10<sup>\", ticksuffix=\"</sup>\")\n",
    "\n",
    "\n",
    "def percentage_y_axis(fig):\n",
    "    fig.layout.yaxis.tickformat = \",.0%\"\n",
    "\n",
    "\n",
    "def format_percentage(value):\n",
    "    return str(round(value * 100, 2)) + \"%\"\n",
    "\n",
    "\n",
    "def committer_date_labels(dict={}):\n",
    "    return {\"committer_date\": \"Year<br><sup>First Release in Year</sup>\"} | dict\n",
    "\n",
    "\n",
    "def revision_labels(dict={}):\n",
    "    return {\"revision\": \"Year\"} | dict\n",
    "\n",
    "\n",
    "def style_legend(fig, position=\"topleft\", xshift=0, yshift=0):\n",
    "    if position == \"topleft\":\n",
    "        fig.update_layout(\n",
    "            legend=dict(yanchor=\"top\", y=0.98 + yshift, xanchor=\"left\", x=0.01 + xshift)\n",
    "        )\n",
    "    elif position == \"topright\":\n",
    "        fig.update_layout(\n",
    "            legend=dict(\n",
    "                yanchor=\"top\", y=0.98 + yshift, xanchor=\"right\", x=0.98 + xshift\n",
    "            )\n",
    "        )\n",
    "    elif position == \"bottomright\":\n",
    "        fig.update_layout(\n",
    "            legend=dict(\n",
    "                yanchor=\"bottom\", y=0.01 + yshift, xanchor=\"right\", x=0.98 + xshift\n",
    "            )\n",
    "        )\n",
    "    elif position == \"bottomleft\":\n",
    "        fig.update_layout(\n",
    "            legend=dict(\n",
    "                yanchor=\"bottom\", y=0.01 + yshift, xanchor=\"left\", x=0.01 + xshift\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        fig.update_layout(showlegend=False)\n",
    "\n",
    "\n",
    "def style_box(fig, legend_position=\"topleft\", xshift=0, yshift=0):\n",
    "    fig.update_traces(fillcolor=\"rgba(0,0,0,0)\")\n",
    "    fig.update_traces(line_width=1)\n",
    "    fig.update_traces(marker_size=2)\n",
    "    fig.update_layout(font_family=\"Linux Biolinum\")\n",
    "    style_legend(fig, legend_position, xshift, yshift)\n",
    "\n",
    "\n",
    "def style_scatter(fig, marker_size=4, legend_position=\"topleft\", xshift=0, yshift=0):\n",
    "    if marker_size:\n",
    "        fig.update_traces(marker_size=marker_size)\n",
    "    style_legend(fig, legend_position, xshift, yshift)\n",
    "    fig.update_layout(font_family=\"Linux Biolinum\")\n",
    "\n",
    "\n",
    "def plot_failures(\n",
    "    fig, df, x, y, y_value, align=\"bottom\", xref=\"x\", font_size=10, textangle=270\n",
    "):\n",
    "    group = df.groupby(x, dropna=False)\n",
    "    failures = (\n",
    "        (group[y].size() - group[y].count())\n",
    "        .reset_index()\n",
    "        .rename(columns={y: f\"{y}_failures\"})\n",
    "    )\n",
    "    attempts = group[y].size().reset_index().rename(columns={y: f\"{y}_attempts\"})\n",
    "    failures = pd.merge(failures, attempts)\n",
    "    failures[f\"{y}_text\"] = (\n",
    "        failures[f\"{y}_failures\"].astype(str)\n",
    "        + \" (\"\n",
    "        + (failures[f\"{y}_failures\"] / failures[f\"{y}_attempts\"]).apply(\n",
    "            lambda v: \"{0:.1f}%\".format(v * 100)\n",
    "        )\n",
    "        + \")\"\n",
    "    )\n",
    "    for row in range(len(failures)):\n",
    "        text = failures.at[row, f\"{y}_text\"]\n",
    "        text = \"\" if failures.at[row, f\"{y}_failures\"] == 0 else text\n",
    "        fig.add_annotation(\n",
    "            x=failures.at[row, x],\n",
    "            y=y_value,\n",
    "            text=text,\n",
    "            showarrow=False,\n",
    "            font_size=font_size,\n",
    "            textangle=textangle,\n",
    "            align=\"left\" if align == \"bottom\" else \"right\",\n",
    "            yanchor=\"bottom\" if align == \"bottom\" else \"top\",\n",
    "            yshift=5 if align == \"bottom\" else -5,\n",
    "            font_color=\"gray\",\n",
    "            xref=xref,\n",
    "        )\n",
    "\n",
    "\n",
    "def cohens_d(d1, d2):\n",
    "    # uses pooled standard deviation\n",
    "    n1, n2 = len(d1), len(d2)\n",
    "    s1, s2 = np.var(d1, ddof=1), np.var(d2, ddof=1)\n",
    "    s = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    "    u1, u2 = np.mean(d1), np.mean(d2)\n",
    "    return (u1 - u2) / s\n",
    "\n",
    "\n",
    "def wilcoxon_test(df, column_a, column_b):\n",
    "    # if the same values are returned for many inputs, refer to https://stats.stackexchange.com/q/232927\n",
    "    a = df[column_a][~df[column_a].isna()]\n",
    "    b = df[column_b][~df[column_b].isna()]\n",
    "    d = a - b\n",
    "    results = scipy.stats.wilcoxon(d, method=\"approx\")\n",
    "    p = results.pvalue\n",
    "    # adapted from https://stats.stackexchange.com/q/133077\n",
    "    r = np.abs(results.zstatistic / np.sqrt(len(d) * 2))\n",
    "    return p, r\n",
    "\n",
    "\n",
    "def style_p_values(\n",
    "    fig, brackets, scale=0, _format=dict(interline=0.07, text_height=1.07, color=\"gray\")\n",
    "):\n",
    "    # adapted from https://stackoverflow.com/q/67505252\n",
    "    for entry in brackets:\n",
    "        first_column, second_column, y, results = entry\n",
    "        y_range = [1.01 + y * _format[\"interline\"], 1.02 + y * _format[\"interline\"]]\n",
    "        p, r = results\n",
    "        if p >= 0.05:\n",
    "            symbol = \"ns\"\n",
    "        elif p >= 0.01:\n",
    "            symbol = \"*\"\n",
    "        elif p >= 0.001:\n",
    "            symbol = \"**\"\n",
    "        else:\n",
    "            symbol = \"***\"\n",
    "        first_column = first_column - scale\n",
    "        second_column = second_column + scale\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            xref=\"x\",\n",
    "            yref=\"y domain\",\n",
    "            x0=first_column,\n",
    "            y0=y_range[0],\n",
    "            x1=first_column,\n",
    "            y1=y_range[1],\n",
    "            line=dict(\n",
    "                color=_format[\"color\"],\n",
    "                width=2,\n",
    "            ),\n",
    "        )\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            xref=\"x\",\n",
    "            yref=\"y domain\",\n",
    "            x0=first_column,\n",
    "            y0=y_range[1],\n",
    "            x1=second_column,\n",
    "            y1=y_range[1],\n",
    "            line=dict(\n",
    "                color=_format[\"color\"],\n",
    "                width=2,\n",
    "            ),\n",
    "        )\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            xref=\"x\",\n",
    "            yref=\"y domain\",\n",
    "            x0=second_column,\n",
    "            y0=y_range[0],\n",
    "            x1=second_column,\n",
    "            y1=y_range[1],\n",
    "            line=dict(\n",
    "                color=_format[\"color\"],\n",
    "                width=2,\n",
    "            ),\n",
    "        )\n",
    "        fig.add_annotation(\n",
    "            dict(\n",
    "                font=dict(color=_format[\"color\"], size=14),\n",
    "                x=(first_column + second_column) / 2,\n",
    "                y=y_range[1] * _format[\"text_height\"],\n",
    "                showarrow=False,\n",
    "                text=symbol + \" <sup>(\" + str(round(r, 2)) + \")</sup>\",\n",
    "                textangle=0,\n",
    "                xref=\"x\",\n",
    "                yref=\"y domain\",\n",
    "            )\n",
    "        )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def bracket_for(i, j, xshift, y, results):\n",
    "    return [i + xshift, j + xshift, y, results]\n",
    "\n",
    "\n",
    "def filter_extractor(df, extractor):\n",
    "    return df[df[\"extractor\"] == extractor]\n",
    "\n",
    "\n",
    "def annotate_value(\n",
    "    fig,\n",
    "    x,\n",
    "    y,\n",
    "    subplot,\n",
    "    prefix,\n",
    "    ax,\n",
    "    ay,\n",
    "    xanchor,\n",
    "    df,\n",
    "    fn=lambda prefix, y: prefix + \": \" + format(round(y), \",\") if y > 0 else prefix,\n",
    "):\n",
    "    if df.empty:\n",
    "        return\n",
    "    if isinstance(x, str):\n",
    "        x = df[x].iat[0]\n",
    "    if isinstance(y, str):\n",
    "        y = df[y].iat[0]\n",
    "    fig.add_annotation(\n",
    "        xref=\"x\" + str(subplot),\n",
    "        yref=\"y\" + str(subplot),\n",
    "        x=x,\n",
    "        y=y,\n",
    "        ax=ax,\n",
    "        ay=ay,\n",
    "        xanchor=xanchor,\n",
    "        text=fn(prefix, y),\n",
    "    )\n",
    "\n",
    "\n",
    "def show(fig, name=None, width=1000, height=500, margin=None):\n",
    "    # fig.update_layout(width=width, height=height)\n",
    "    if margin:\n",
    "        fig.update_layout(margin=margin)\n",
    "    else:\n",
    "        fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "\n",
    "    # if figures_directory and os.path.isdir(figures_directory) and name:\n",
    "    # fig.write_image(f'{figures_directory}/{name}.pdf')\n",
    "    # fig.write_html(f'{figures_directory}/{name}.html',config={\"responsive\":True})\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_arch(df):\n",
    "    grouped = df.groupby(\"architecture\")\n",
    "    dfs = {arch: group for arch, group in grouped}\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def read_dataframe_linux(stage, dtype={}, usecols=None, file=None, arch=None):\n",
    "    if not file:\n",
    "        file = \"output\"\n",
    "    df = pd.read_csv(f\"output-linux/{stage}/{file}.csv\", dtype=dtype, usecols=usecols)\n",
    "    if \"committer_date_unix\" in df:\n",
    "        df[\"committer_date\"] = df[\"committer_date_unix\"].apply(\n",
    "            lambda d: pd.to_datetime(d, unit=\"s\")\n",
    "        )\n",
    "    if arch != None:\n",
    "        return group_by_arch(df)[arch]\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_values(df):\n",
    "    df.replace(\"kconfigreader\", \"KConfigReader\", inplace=True)\n",
    "    df.replace(\"kmax\", \"KClause\", inplace=True)\n",
    "\n",
    "\n",
    "def big_log10(str):\n",
    "    return log10(int(str)) if not pd.isna(str) and str != \"\" else pd.NA\n",
    "\n",
    "\n",
    "def process_model_count(df_solve):\n",
    "    df_solve[\"model-count\"] = df_solve[\"model-count\"].replace(\"1\", \"\")\n",
    "    df_solve[\"model-count-log10\"] = (\n",
    "        df_solve[\"model-count\"].fillna(\"\").apply(big_log10).replace(0, np.nan)\n",
    "    )\n",
    "    df_solve[\"year\"] = df_solve[\"committer_date\"].apply(lambda d: int(d.year))\n",
    "\n",
    "\n",
    "def peek_dataframe(\n",
    "    df, column, message, type=\"str\", filter=[\"revision\", \"architecture\", \"extractor\"]\n",
    "):\n",
    "    success = df[\n",
    "        ~df[column].str.contains(\"NA\") if type == \"str\" else ~df[column].isna()\n",
    "    ][filter]\n",
    "    failure = df[df[column].str.contains(\"NA\") if type == \"str\" else df[column].isna()][\n",
    "        filter\n",
    "    ]\n",
    "    print(f\"{message}: {len(success)} successes, {len(failure)} failures\")\n",
    "\n",
    "\n",
    "def jaccard(a, b):\n",
    "    return len(set.intersection(a, b)) / len(set.union(a, b))\n",
    "\n",
    "\n",
    "def add_features(descriptor, source, features, min=2):\n",
    "    descriptor[f\"#{source}\"] = (\n",
    "        len(features) if features is not None and len(features) >= min else np.nan\n",
    "    )\n",
    "\n",
    "\n",
    "def get_variables(variable_map):\n",
    "    variables = set(variable_map.values())\n",
    "    if len(variables) <= 1:\n",
    "        variables = set()\n",
    "    return variables\n",
    "def number_of_models(df):\n",
    "    return len(df[['revision','architecture', 'extractor']].drop_duplicates())\n",
    "\n",
    "def unify_solvers(df, columns=['model-count-unconstrained-log10']):\n",
    "    return df[['revision', 'committer_date', 'architecture', 'extractor', *columns]].drop_duplicates()\n",
    "\n",
    "def is_accurate(series):\n",
    "    return len(set.difference(set(series), {pd.NA})) < 2\n",
    "\n",
    "def big_sum(series):\n",
    "    big_sum = sum([int(value) for value in series if not pd.isna(value) and value])\n",
    "    if big_sum > 0:\n",
    "        return len(str(big_sum))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_for(df, column, committer_date):\n",
    "    x = df.sort_values(by=[committer_date])\n",
    "    return x.tail(1)[column]\n",
    "\n",
    "\n",
    "def by_revision(df):\n",
    "    x = df[df[\"revision\"].str.contains(\"\\w\\d+\\.0$\", regex=True)]\n",
    "    if len(x) == 0:\n",
    "        x = df.sort_values(by=[\"revision\"])\n",
    "    return x\n",
    "\n",
    "\n",
    "def find_revision(df, revision):\n",
    "    x = df[df[\"revision\"].str.contains(revision, regex=False)]\n",
    "    return x\n",
    "\n",
    "\n",
    "def for_arch(df, arch):\n",
    "    return df[df[\"architecture\"] == arch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_object_to_file(obj, name):\n",
    "    with open(name, \"w\") as fp:\n",
    "        json.dump(obj, fp)\n",
    "\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path) as json_data:\n",
    "        return json.load(json_data)\n",
    "\n",
    "\n",
    "def merge_metrics(new):\n",
    "    old = read_json(init_json_path)\n",
    "\n",
    "    for proj, metrics in new.items():\n",
    "        for metric, values in metrics.items():\n",
    "            # print(f\"{proj=}, {metric=}, {values=}\")\n",
    "            for name, value in values.items():\n",
    "                if proj not in old[\"projectData\"]:\n",
    "                    print(f\"{proj} not in old\")\n",
    "                    continue\n",
    "                old[\"projectData\"][proj][metric][name] = value\n",
    "    write_object_to_file(old, init_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linux:\n",
    "    def read_dataframe(self, stage, dtype={}, usecols=None, file=None):\n",
    "        if not file:\n",
    "            file = \"output\"\n",
    "        df = pd.read_csv(\n",
    "            f\"{self.output_directory}/{stage}/{file}.csv\", dtype=dtype, usecols=usecols\n",
    "        )\n",
    "        if \"committer_date_unix\" in df:\n",
    "            df[\"committer_date\"] = df[\"committer_date_unix\"].apply(\n",
    "                lambda d: pd.to_datetime(d, unit=\"s\")\n",
    "            )\n",
    "        return df\n",
    "    def solver_successes(self, solver):\n",
    "        df_solve_for_solver = self.df_solve_attempts[~self.df_solve_attempts['model-count'].isna()]\n",
    "        df_solve_for_solver = df_solve_for_solver[df_solve_for_solver['backbone.dimacs-analyzer'] == solver]\n",
    "        return set(df_solve_for_solver['extractor'] + ',' + df_solve_for_solver['revision'] + ',' + df_solve_for_solver['architecture'])\n",
    "\n",
    "    def __init__(self):\n",
    "        self.output_directory = \"output-linux\"\n",
    "        self.df_kconfig = self.read_dataframe(\"kconfig\")\n",
    "        self.df_kconfig[\"year\"] = self.df_kconfig[\"committer_date\"].apply(\n",
    "            lambda d: int(d.year)\n",
    "        )\n",
    "        self.df_architectures = self.read_dataframe(\"read-linux-architectures\")\n",
    "        self.df_architectures = self.df_architectures.sort_values(by=\"committer_date\")\n",
    "        self.df_architectures[\"year\"] = self.df_architectures[\"committer_date\"].apply(\n",
    "            lambda d: int(d.year)\n",
    "        )\n",
    "        self.df_configs = self.read_dataframe(\"read-linux-configs\")\n",
    "        self.df_configs = self.df_configs[\n",
    "            ~self.df_configs[\"kconfig-file\"].str.contains(\"/um/\")\n",
    "        ]\n",
    "        self.df_config_types = self.read_dataframe(\n",
    "            \"read-linux-configs\", file=\"output.types\"\n",
    "        )\n",
    "        self.df_config_types = self.df_config_types[\n",
    "            ~self.df_config_types[\"kconfig-file\"].str.contains(\"/um/\")\n",
    "        ]\n",
    "        self.df_config_types = self.df_config_types.merge(\n",
    "            self.df_architectures[[\"revision\", \"committer_date\"]].drop_duplicates()\n",
    "        )\n",
    "        self.df_uvl = self.read_dataframe(\"model_to_uvl_featureide\")\n",
    "        self.df_smt = self.read_dataframe(\"model_to_smt_z3\")\n",
    "        self.df_dimacs = self.read_dataframe(\"dimacs\")\n",
    "        self.df_backbone_dimacs = self.read_dataframe(\"backbone-dimacs\")\n",
    "        self.df_solve = self.read_dataframe(\n",
    "            \"solve_model-count\", {\"model-count\": \"string\"}\n",
    "        )\n",
    "        for df in [\n",
    "            self.df_kconfig,\n",
    "            self.df_uvl,\n",
    "            self.df_smt,\n",
    "            self.df_dimacs,\n",
    "            self.df_backbone_dimacs,\n",
    "            self.df_solve,\n",
    "        ]:\n",
    "            df.replace(\"kconfigreader\", \"KConfigReader\", inplace=True)\n",
    "            df.replace(\"kmax\", \"KClause\", inplace=True)\n",
    "        self.df_configs_configurable = self.df_configs.copy()\n",
    "        self.df_configs_configurable[\"configurable\"] = False\n",
    "        with open(f\"{self.output_directory}/linux-features.dat\", \"rb\") as f:\n",
    "            [\n",
    "                self.features_by_kind_per_architecture,\n",
    "                self.df_extractor_comparison,\n",
    "                self.potential_misses_grep,\n",
    "                self.potential_misses_kmax,\n",
    "                self.df_configs_configurable,\n",
    "            ] = pickle.load(f)\n",
    "\n",
    "        replace_values(self.features_by_kind_per_architecture)\n",
    "        self.df_features = pd.merge(\n",
    "            self.df_architectures, self.features_by_kind_per_architecture, how=\"outer\"\n",
    "        ).sort_values(by=\"committer_date\")\n",
    "        self.df_features = pd.merge(\n",
    "            self.df_kconfig, self.df_features, how=\"outer\"\n",
    "        ).sort_values(by=\"committer_date\")\n",
    "        self.df_total_features = (\n",
    "            self.df_features.groupby([\"extractor\", \"revision\"])\n",
    "            .agg({\"#total_features\": \"min\"})\n",
    "            .reset_index()\n",
    "        )\n",
    "        self.df_total_features = pd.merge(\n",
    "            self.df_kconfig[[\"committer_date\", \"revision\"]].drop_duplicates(),\n",
    "            self.df_total_features,\n",
    "        )\n",
    "        self.metrics = dict()\n",
    "\n",
    "    def solver_dfs(self):\n",
    "        df_solve_unconstrained = self.df_solve.merge(self.df_features)\n",
    "        df_solve_unconstrained[\"model-count-unconstrained\"] = df_solve_unconstrained.apply(\n",
    "            lambda row: str(\n",
    "                int(row[\"model-count\"])\n",
    "                * (2 ** int(row[\"unconstrained_bools\"]))\n",
    "                * (3 ** int(row[\"unconstrained_tristates\"]))\n",
    "            )\n",
    "            if not pd.isna(row[\"model-count\"]) and row[\"model-count\"] != \"\"\n",
    "            else pd.NA,\n",
    "            axis=1,\n",
    "        )\n",
    "        df_solve_unconstrained[\"model-count-unconstrained-log10\"] = (\n",
    "            df_solve_unconstrained[\"model-count-unconstrained\"]\n",
    "            .fillna(\"\")\n",
    "            .map(big_log10)\n",
    "            .replace(0, np.nan)\n",
    "        )\n",
    "        df_solve_unconstrained[\"similarity\"] = df_solve_unconstrained.apply(\n",
    "            lambda row: int(row[\"model-count\"]) / int(row[\"model-count-unconstrained\"])\n",
    "            if not pd.isna(row[\"model-count\"]) and row[\"model-count\"] != \"\"\n",
    "            else pd.NA,\n",
    "            axis=1,\n",
    "        )\n",
    "        df_solve_extractor_comparison = pd.pivot(\n",
    "            df_solve_unconstrained[\n",
    "                [\"revision\", \"architecture\", \"extractor\", \"model-count-unconstrained-log10\"]\n",
    "            ]\n",
    "            .dropna()\n",
    "            .drop_duplicates(),\n",
    "            index=[\"revision\", \"architecture\"],\n",
    "            columns=\"extractor\",\n",
    "        ).dropna()\n",
    "        print(df_solve_extractor_comparison)\n",
    "        df_solve_extractor_comparison = (\n",
    "            df_solve_extractor_comparison[\"model-count-unconstrained-log10\"][\"KConfigReader\"]\n",
    "            / df_solve_extractor_comparison[\"model-count-unconstrained-log10\"][\"KClause\"]\n",
    "        )\n",
    "\n",
    "        def unify_solvers(df, columns=['model-count-unconstrained-log10']):\n",
    "            return df[['revision', 'committer_date', 'architecture', 'extractor', *columns]].drop_duplicates()\n",
    "\n",
    "        def is_accurate(series):\n",
    "            return len(set.difference(set(series), {pd.NA})) < 2\n",
    "\n",
    "        def big_sum(series):\n",
    "            big_sum = sum([int(value) for value in series if not pd.isna(value) and value])\n",
    "            if big_sum > 0:\n",
    "                return len(str(big_sum))\n",
    "            \n",
    "        df_solve_inaccuracies = df_solve_unconstrained.groupby(['extractor', 'revision', 'architecture']).agg({'model-count': is_accurate})\n",
    "        df_solve_inaccuracies = df_solve_inaccuracies.dropna()\n",
    "\n",
    "        self.df_solve_slice = df_solve_unconstrained[df_solve_unconstrained['year'] <= 2013]\n",
    "        self.df_solve_failures = self.df_solve_slice.groupby(['extractor', 'revision', 'architecture'], dropna=False).agg({'model-count-unconstrained-log10': lambda x: (True in list(pd.notna(x)) or pd.NA)}).reset_index()\n",
    "        self.df_solve_group = self.df_solve_failures.groupby(['extractor', 'revision'], dropna=False)\n",
    "        self.df_solve_failures = (self.df_solve_group['model-count-unconstrained-log10'].size() - self.df_solve_group['model-count-unconstrained-log10'].count()).reset_index()\n",
    "        self.df_solve_failures['is-upper-bound'] = self.df_solve_failures['model-count-unconstrained-log10'] == 0\n",
    "        self.df_solve_failures = self.df_solve_failures.rename(columns={'model-count-unconstrained-log10': 'failures'})\n",
    "        self.df_solve_total = unify_solvers(pd.merge(self.df_solve_slice, self.df_solve_failures), ['model-count-unconstrained', 'model-count-unconstrained-log10', 'is-upper-bound', 'failures', 'year'])\n",
    "        self.df_solve_total = self.df_solve_total.groupby(['extractor', 'committer_date', 'year']).agg({'model-count-unconstrained': big_sum, 'is-upper-bound': 'min', 'failures': 'min'}).reset_index()\n",
    "    \n",
    "    def model_count_latest(self):\n",
    "        archs = list(self.df_kconfig[\"architecture\"].unique())\n",
    "        archs.append(\"all\")\n",
    "        for arch in archs:\n",
    "            df_arch = for_arch(self.df_solve_slice, arch)\n",
    "            key = \"model-count-unconstrained\"\n",
    "            if arch == \"all\":\n",
    "                df_arch = self.df_solve_total\n",
    "                key = \"model-count-unconstrained-log10\"\n",
    "            df_arch = by_revision(df_arch)\n",
    "            sloc = int(latest_for(df_arch, key, \"committer_date_unix\").iloc[0])\n",
    "            last_rev = latest_for(df_arch, \"revision\", \"committer_date_unix\").iloc[0]\n",
    "            major = int(last_rev[1])\n",
    "            before_last = df_arch[\n",
    "                df_arch[\"revision\"].str.contains(f\"\\w{major - 1}\\.\\d$\", regex=True)\n",
    "            ]\n",
    "            if len(before_last) == 0:\n",
    "                self.metrics[f\"linux/{arch}\"] = {\n",
    "                    \"model-count\": {\n",
    "                        \"currentValue\": sloc,\n",
    "                        \"cmpLastRevision\": \"+100% (No Prior Revision)\",\n",
    "                    }\n",
    "                }\n",
    "                continue\n",
    "            before_last = before_last[key]\n",
    "            before_last = int(before_last.iloc[0])\n",
    "            value = round(100 * (sloc - before_last) / before_last, 2)\n",
    "            self.metrics[f\"linux/{arch}\"] = {\n",
    "                \"model-count\": {\n",
    "                    \"currentValue\": f\"{sloc} loc\",\n",
    "                    \"cmpLastRevision\": f\"{value:+.1f}%\",\n",
    "                }\n",
    "            }\n",
    "\n",
    "    def fill_metrics(self):\n",
    "        self.total_features_latest()\n",
    "        self.features_latest()\n",
    "        self.sloc_latest()\n",
    "        self.model_count_latest()\n",
    "        merge_metrics(self.metrics)\n",
    "\n",
    "    def total_features_latest(self):\n",
    "        kclause = int(\n",
    "            self.df_total_features[self.df_total_features[\"extractor\"] == \"KClause\"]\n",
    "            .sort_values(\"committer_date\")\n",
    "            .tail(1)[\"#total_features\"]\n",
    "        )\n",
    "        kconf = int(\n",
    "            self.df_total_features[\n",
    "                self.df_total_features[\"extractor\"] == \"KConfigReader\"\n",
    "            ]\n",
    "            .sort_values(\"committer_date\")\n",
    "            .tail(1)\n",
    "            .iloc[0][\"#total_features\"]\n",
    "        )\n",
    "        self.metrics[\"linux/all\"] = {\n",
    "            \"total-features\": {\n",
    "                \"currentValue\": f\"KClause: {kclause}\\nKConfigReader: {kconf}\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def features_latest(self):\n",
    "        archs = list(self.df_kconfig[\"architecture\"].unique())\n",
    "        for architecture in archs:\n",
    "            df = for_arch(self.df_features, architecture)\n",
    "            kclause = int(\n",
    "                df[df[\"extractor\"] == \"KClause\"]\n",
    "                .sort_values(\"committer_date\")\n",
    "                .tail(1)[\"#features\"]\n",
    "            )\n",
    "            kconf = int(\n",
    "                df[df[\"extractor\"] == \"KConfigReader\"]\n",
    "                .sort_values(\"committer_date\")\n",
    "                .tail(1)\n",
    "                .iloc[0][\"#features\"]\n",
    "            )\n",
    "            self.metrics[f\"linux/{architecture}\"] = {\n",
    "                \"features\": {\n",
    "                    \"currentValue\": f\"KClause: {kclause} features\\nKConfigReader: {kconf} features\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "    def sloc_latest(self):\n",
    "        archs = list(self.df_kconfig[\"architecture\"].unique())\n",
    "        archs.append(\"all\")\n",
    "        for arch in archs:\n",
    "            df_arch = for_arch(self.df_kconfig, arch)\n",
    "            if arch == \"all\":\n",
    "                df_arch = self.df_kconfig\n",
    "            df_arch = by_revision(df_arch)\n",
    "            sloc = int(\n",
    "                latest_for(df_arch, \"source_lines_of_code\", \"committer_date_unix\").iloc[\n",
    "                    0\n",
    "                ]\n",
    "            )\n",
    "            last_rev = latest_for(df_arch, \"revision\", \"committer_date_unix\").iloc[0]\n",
    "            major = int(last_rev[1])\n",
    "            before_last = df_arch[\n",
    "                df_arch[\"revision\"].str.contains(f\"\\w{major - 1}\\.\\d$\", regex=True)\n",
    "            ]\n",
    "            if len(before_last) == 0:\n",
    "                self.metrics[f\"linux/{arch}\"] = {\n",
    "                    \"source_lines_of_code\": {\n",
    "                        \"currentValue\": sloc,\n",
    "                        \"cmpLastRevision\": \"+100% (No Prior Revision)\",\n",
    "                    }\n",
    "                }\n",
    "                continue\n",
    "            before_last = before_last[\"source_lines_of_code\"]\n",
    "            before_last = int(before_last.iloc[0])\n",
    "            value = round(100 * (sloc - before_last) / before_last, 2)\n",
    "            self.metrics[f\"linux/{arch}\"] = {\n",
    "                \"source_lines_of_code\": {\n",
    "                    \"currentValue\": f\"{sloc} loc\",\n",
    "                    \"cmpLastRevision\": f\"{value:+.1f}%\",\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "linux_dfs = Linux()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlinux_dfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolver_dfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 90\u001b[0m, in \u001b[0;36mLinux.solver_dfs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msolver_dfs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     89\u001b[0m     df_solve_unconstrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_solve\u001b[38;5;241m.\u001b[39mmerge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_features)\n\u001b[0;32m---> 90\u001b[0m     df_solve_unconstrained[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count-unconstrained\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_solve_unconstrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel-count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munconstrained_bools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munconstrained_tristates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel-count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel-count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     df_solve_unconstrained[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count-unconstrained-log10\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    101\u001b[0m         df_solve_unconstrained[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count-unconstrained\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;241m.\u001b[39mmap(big_log10)\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m     df_solve_unconstrained[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_solve_unconstrained\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count-unconstrained\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mNA,\n\u001b[1;32m    110\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    111\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/torte/lib/python3.10/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/torte/lib/python3.10/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/torte/lib/python3.10/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/.pyenv/versions/torte/lib/python3.10/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[39], line 93\u001b[0m, in \u001b[0;36mLinux.solver_dfs.<locals>.<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msolver_dfs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     89\u001b[0m     df_solve_unconstrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_solve\u001b[38;5;241m.\u001b[39mmerge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_features)\n\u001b[1;32m     90\u001b[0m     df_solve_unconstrained[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count-unconstrained\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_solve_unconstrained\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 93\u001b[0m             \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munconstrained_bools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munconstrained_tristates\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mNA,\n\u001b[1;32m     98\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     df_solve_unconstrained[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count-unconstrained-log10\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    101\u001b[0m         df_solve_unconstrained[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count-unconstrained\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;241m.\u001b[39mmap(big_log10)\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m     df_solve_unconstrained[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_solve_unconstrained\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count-unconstrained\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mNA,\n\u001b[1;32m    110\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    111\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "linux_dfs.solver_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40821/2858258366.py:262: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  kclause = int(\n",
      "/tmp/ipykernel_40821/2858258366.py:285: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  kclause = int(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'NAType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlinux_dfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 258\u001b[0m, in \u001b[0;36mLinux.fill_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_latest()\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msloc_latest()\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_count_latest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m merge_metrics(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics)\n",
      "Cell \u001b[0;32mIn[29], line 230\u001b[0m, in \u001b[0;36mLinux.model_count_latest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-count-unconstrained-log10\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m df_arch \u001b[38;5;241m=\u001b[39m by_revision(df_arch)\n\u001b[0;32m--> 230\u001b[0m sloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlatest_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_arch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcommitter_date_unix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m last_rev \u001b[38;5;241m=\u001b[39m latest_for(df_arch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommitter_date_unix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    232\u001b[0m major \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(last_rev[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'NAType'"
     ]
    }
   ],
   "source": [
    "linux_dfs.fill_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_sloc_nonLinux(project, vals={}):\n",
    "    output_directory = \"output-busybox\"\n",
    "    df = read_dataframe(\"kconfig\", output_dir=output_directory)\n",
    "    df = df[df[\"system\"] == \"busybox\"]\n",
    "    df_arch = by_revision(df)\n",
    "    lastTwo = df_arch.sort_values(by=\"committer_date_unix\").tail(2)[\"revision\"]\n",
    "    last_rev = lastTwo.iloc[1]\n",
    "    before_last_rev = lastTwo.iloc[0]\n",
    "    sloc = int(df_arch[df_arch[\"revision\"] == last_rev][\"source_lines_of_code\"].iloc[0])\n",
    "    before_last = int(\n",
    "        df_arch[df_arch[\"revision\"] == before_last_rev][\"source_lines_of_code\"].iloc[0]\n",
    "    )\n",
    "    value = round(100 * (sloc - before_last) / before_last, 2)\n",
    "    vals[project] = {\n",
    "        \"source_lines_of_code\": {\n",
    "            \"currentValue\": f\"{sloc} loc\",\n",
    "            \"cmpLastRevision\": f\"{value:+.1f}%\",\n",
    "        }\n",
    "    }\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_total_features_nonLinux(project, vals={}):\n",
    "    output_directory = \"output-busybox\"\n",
    "    df = read_dataframe(\"kconfig\", output_dir=output_directory)\n",
    "    df = df[df[\"system\"] == project]\n",
    "    total_features = latest_for(df, \"model-features\", \"committer_date_unix\").iloc[0]\n",
    "    vals[project] = {\n",
    "        \"total-features\": {\n",
    "            \"currentValue\": f\"{int(total_features)} features\",\n",
    "        }\n",
    "    }\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busybox():\n",
    "    vals = {}\n",
    "    vals = get_metrics_sloc_nonLinux(\"busybox\", vals)\n",
    "    vals = get_metrics_total_features_nonLinux(\"busybox\", vals)\n",
    "    merge_metrics(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linux():\n",
    "    vals = {}\n",
    "    vals = get_metrics_sloc_linux"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
