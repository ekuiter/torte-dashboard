{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install plotly pandas statsmodels kaleido scipy nbformat jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pickle\n",
    "import scipy\n",
    "from statistics import mean, stdev\n",
    "from math import sqrt, log10\n",
    "from packaging.version import Version\n",
    "\n",
    "output_directory = \"output-busybox\"\n",
    "base_file = \"init_base.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_file) as json_data:\n",
    "    d = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_nonLinux(df):\n",
    "    # assumes df_kconfig\n",
    "    return df.loc[df['committer_date_unix'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(stage, dtype={}, usecols=None, file=None, project=None):\n",
    "    if not file:\n",
    "        file = 'output'\n",
    "    directory = output_directory\n",
    "    if project != None:\n",
    "        directory = f\"output-{project}\"\n",
    "    df = pd.read_csv(f'{directory}/{stage}/{file}.csv', dtype=dtype, usecols=usecols)\n",
    "    if 'committer_date_unix' in df:\n",
    "        df['committer_date'] = df['committer_date_unix'].apply(lambda d: pd.to_datetime(d, unit='s'))\n",
    "    return df\n",
    "\n",
    "df_kconfig = read_dataframe('kconfig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(209492.0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sloc = get_latest_nonLinux(df_kconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column_mapping = {\n",
    "    \"features\": \"model-features\",\n",
    "    \"total-features\": \"model-features\",\n",
    "    \"model-count\": \"model-literals\",\n",
    "    \"model-count-time\": \"model-time\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "latestData = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'busybox': {'features': {'cmpLastMonth': '',\n",
      "                          'cmpLastWeek': '',\n",
      "                          'currentValue': '1138.0',\n",
      "                          'desc': 'Features: Information, Description etc.',\n",
      "                          'title': 'Features'},\n",
      "             'model-count': {'cmpLastMonth': '',\n",
      "                             'cmpLastWeek': '',\n",
      "                             'currentValue': '5200.0',\n",
      "                             'desc': 'Model Count: Information, Description '\n",
      "                                     'etc.',\n",
      "                             'title': 'Model Count'},\n",
      "             'model-count-time': {'cmpLastMonth': '',\n",
      "                                  'cmpLastWeek': '',\n",
      "                                  'currentValue': '7745479537',\n",
      "                                  'desc': 'Model Count Time: Information, '\n",
      "                                          'Description etc.',\n",
      "                                  'title': 'Model Count Time'},\n",
      "             'source_lines_of_code': {'cmpLastMonth': '',\n",
      "                                      'cmpLastWeek': '',\n",
      "                                      'currentValue': '209492.0',\n",
      "                                      'desc': 'Source Lines of Code: '\n",
      "                                              'Information, Description etc.',\n",
      "                                      'title': 'Source Lines of Code'},\n",
      "             'total-features': {'cmpLastMonth': '',\n",
      "                                'cmpLastWeek': '',\n",
      "                                'currentValue': '1138.0',\n",
      "                                'desc': 'Total Features: Information, '\n",
      "                                        'Description etc.',\n",
      "                                'title': 'Total Features'}}}\n"
     ]
    }
   ],
   "source": [
    "def fill_latest_nonLinux():\n",
    "    for bp in d:\n",
    "        if bp == \"linux\":\n",
    "            continue\n",
    "        projects = d[bp][\"projects\"]\n",
    "        plots = d[bp][\"plots\"]\n",
    "        for p in projects:\n",
    "            if p not in latestData:\n",
    "                latestData[p] = dict()\n",
    "            df_kconfig = read_dataframe('kconfig')\n",
    "            latest_all = get_latest_nonLinux(df_kconfig[df_kconfig[\"system\"] == p])\n",
    "            for plot in plots:\n",
    "                latest = latest_all[plot_column_mapping.get(plot, plot)]\n",
    "                latestData[p][plot] = {\n",
    "                    \"title\": d[bp][\"plots\"][plot][\"title\"],\n",
    "                    \"currentValue\": str(latest),\n",
    "                    \"cmpLastWeek\": \"\",\n",
    "                    \"cmpLastMonth\": \"\",\n",
    "                    \"desc\": d[bp][\"plots\"][plot][\"desc\"],\n",
    "                }\n",
    "fill_latest_nonLinux()\n",
    "from pprint import pprint\n",
    "pprint(latestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_latest_linux():\n",
    "    for bp in d:\n",
    "        if bp != \"linux\":\n",
    "            continue\n",
    "        projects = d[bp][\"projects\"]\n",
    "        plots = d[bp][\"plots\"]\n",
    "        for p in projects:\n",
    "            arch = p.rsplit(\"/\")[-1]\n",
    "            if arch == \"all\":\n",
    "                continue\n",
    "            if p not in latestData:\n",
    "                latestData[p] = dict()\n",
    "            df_kconfig = read_dataframe('kconfig', project=\"linux\")\n",
    "            latest_all = get_latest_nonLinux(df_kconfig[df_kconfig[\"architecture\"] == arch])\n",
    "            for plot in plots:\n",
    "                latest = latest_all[plot_column_mapping.get(plot, plot)]\n",
    "                latestData[p][plot] = {\n",
    "                    \"title\": \"Total Features\",\n",
    "                    \"currentValue\": str(latest),\n",
    "                    \"cmpLastWeek\": \"\",\n",
    "                    \"cmpLastMonth\": \"\",\n",
    "                    \"desc\": \"Total Features: Information, Description etc.\",\n",
    "                }\n",
    "fill_latest_linux()\n",
    "# from pprint import pprint\n",
    "# pprint(latestData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_arch(df):\n",
    "    grouped = df.groupby('architecture')\n",
    "    dfs = {arch: group for arch, group in grouped}\n",
    "    return dfs\n",
    "\n",
    "def read_dataframe(stage, dtype={}, usecols=None, file=None, arch=None, output_dir=output_directory):\n",
    "    if not file:\n",
    "        file = 'output'\n",
    "    df = pd.read_csv(f'{output_dir}/{stage}/{file}.csv', dtype=dtype, usecols=usecols)\n",
    "    if 'committer_date_unix' in df:\n",
    "        df['committer_date'] = df['committer_date_unix'].apply(lambda d: pd.to_datetime(d, unit='s'))\n",
    "    if arch != None:\n",
    "        return group_by_arch(df)[arch]\n",
    "    return df\n",
    "\n",
    "def read_dataframe_linux(stage, dtype={}, usecols=None, file=None, arch=None):\n",
    "    if not file:\n",
    "        file = 'output'\n",
    "    df = pd.read_csv(f'output-linux/{stage}/{file}.csv', dtype=dtype, usecols=usecols)\n",
    "    if 'committer_date_unix' in df:\n",
    "        df['committer_date'] = df['committer_date_unix'].apply(lambda d: pd.to_datetime(d, unit='s'))\n",
    "    if arch != None:\n",
    "        return group_by_arch(df)[arch]\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_values(df):\n",
    "    df.replace('kconfigreader', 'KConfigReader', inplace=True)\n",
    "    df.replace('kmax', 'KClause', inplace=True)\n",
    "\n",
    "def big_log10(str):\n",
    "    return log10(int(str)) if not pd.isna(str) and str != '' else pd.NA\n",
    "\n",
    "def process_model_count(df_solve):\n",
    "    df_solve['model-count'] = df_solve['model-count'].replace('1', '')\n",
    "    df_solve['model-count-log10'] = df_solve['model-count'].fillna('').apply(big_log10).replace(0, np.nan)\n",
    "    df_solve['year'] = df_solve['committer_date'].apply(lambda d: int(d.year))\n",
    "\n",
    "def peek_dataframe(df, column, message, type='str', filter=['revision', 'architecture', 'extractor']):\n",
    "    success = df[~df[column].str.contains('NA') if type == 'str' else ~df[column].isna()][filter]\n",
    "    failure = df[df[column].str.contains('NA') if type == 'str' else df[column].isna()][filter]\n",
    "    print(f'{message}: {len(success)} successes, {len(failure)} failures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"output-linux\"\n",
    "df_kconfig = read_dataframe('kconfig')\n",
    "df_kconfig['year'] = df_kconfig['committer_date'].apply(lambda d: int(d.year))\n",
    "\n",
    "df_architectures = read_dataframe(f'read-linux-architectures')\n",
    "df_architectures = df_architectures.sort_values(by='committer_date')\n",
    "df_architectures['year'] = df_architectures['committer_date'].apply(lambda d: int(d.year))\n",
    "\n",
    "df_configs = read_dataframe(f'read-linux-configs')\n",
    "df_configs = df_configs[~df_configs['kconfig-file'].str.contains('/um/')]\n",
    "\n",
    "df_config_types = read_dataframe(f'read-linux-configs', file='output.types')\n",
    "df_config_types = df_config_types[~df_config_types['kconfig-file'].str.contains('/um/')]\n",
    "df_config_types = df_config_types.merge(df_architectures[['revision', 'committer_date']].drop_duplicates())\n",
    "    \n",
    "\n",
    "df_uvl = read_dataframe('model_to_uvl_featureide')\n",
    "df_smt = read_dataframe('model_to_smt_z3')\n",
    "df_dimacs = read_dataframe('dimacs')\n",
    "df_backbone_dimacs = read_dataframe('backbone-dimacs')\n",
    "df_solve = read_dataframe('solve_model-count', {'model-count': 'string'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#potential misses (grep): 7\n",
      "                        config             kconfig-file      type\n",
      "0                         ARCH             init/Kconfig    string\n",
      "53               KERNELVERSION             init/Kconfig    string\n",
      "106                IA64_SGI_UV        arch/ia64/Kconfig      bool\n",
      "187  SND_SOC_UX500_MACH_MOP500  sound/soc/ux500/Kconfig  tristate\n",
      "\n",
      "#potential misses (kmax): 21\n",
      "                                config                        kconfig-file  \\\n",
      "0                      MIPS_FPE_MODULE                 arch/mips64/Kconfig   \n",
      "29                      BLK_DEV_FD1772         drivers/acorn/block/Kconfig   \n",
      "83                         BLK_DEV_MFM         drivers/acorn/block/Kconfig   \n",
      "137             BLK_DEV_MFM_AUTODETECT         drivers/acorn/block/Kconfig   \n",
      "191                      VIRTEX_II_PRO      arch/ppc/platforms/4xx/Kconfig   \n",
      "207                      VIRTEX_II_PRO  arch/powerpc/platforms/4xx/Kconfig   \n",
      "223                          DRAM_BASE              arch/arm/Kconfig-nommu   \n",
      "339                          DRAM_SIZE              arch/arm/Kconfig-nommu   \n",
      "435                     FLASH_MEM_BASE              arch/arm/Kconfig-nommu   \n",
      "531                         FLASH_SIZE              arch/arm/Kconfig-nommu   \n",
      "627               REMAP_VECTORS_TO_RAM              arch/arm/Kconfig-nommu   \n",
      "723                      SET_MEM_PARAM              arch/arm/Kconfig-nommu   \n",
      "819              DUAL_CORE_TEST_MODULE               arch/blackfin/Kconfig   \n",
      "824              DUAL_CORE_TEST_MODULE         arch/blackfin/Kconfig.debug   \n",
      "829                    S3C2410_CPUFREQ       arch/arm/mach-s3c2410/Kconfig   \n",
      "848                   S3C2410_PLLTABLE       arch/arm/mach-s3c2410/Kconfig   \n",
      "865                           FANOTIFY          fs/notify/fanotify/Kconfig   \n",
      "942        FANOTIFY_ACCESS_PERMISSIONS          fs/notify/fanotify/Kconfig   \n",
      "1019                   S3C2410_CPUFREQ       arch/arm/mach-s3c24xx/Kconfig   \n",
      "1038                       TEST_MODULE                   lib/Kconfig.debug   \n",
      "1042  SND_SOC_DM365_VOICE_CODEC_MODULE                sound/soc/ti/Kconfig   \n",
      "1054                 TEST_KASAN_MODULE                   lib/Kconfig.kasan   \n",
      "1056                         DRAM_BASE                   arch/csky/Kconfig   \n",
      "\n",
      "          type  \n",
      "0         bool  \n",
      "29    tristate  \n",
      "83    tristate  \n",
      "137       bool  \n",
      "191       bool  \n",
      "207       bool  \n",
      "223        hex  \n",
      "339        hex  \n",
      "435        hex  \n",
      "531        hex  \n",
      "627       bool  \n",
      "723       bool  \n",
      "819   tristate  \n",
      "824   tristate  \n",
      "829       bool  \n",
      "848       bool  \n",
      "865       bool  \n",
      "942       bool  \n",
      "1019      bool  \n",
      "1038  tristate  \n",
      "1042  tristate  \n",
      "1054  tristate  \n",
      "1056       hex  \n"
     ]
    }
   ],
   "source": [
    "# differentiate kinds of features\n",
    "potential_misses_grep = set()\n",
    "potential_misses_kmax = set()\n",
    "extractor_comparison = {}\n",
    "df_configs_configurable = df_configs.copy()\n",
    "df_configs_configurable['configurable'] = False\n",
    "\n",
    "def jaccard(a, b):\n",
    "    return len(set.intersection(a, b)) / len(set.union(a, b))\n",
    "\n",
    "def add_features(descriptor, source, features, min=2):\n",
    "    descriptor[f'#{source}'] = len(features) if features is not None and len(features) >= min else np.nan\n",
    "\n",
    "def get_variables(variable_map):\n",
    "    variables = set(variable_map.values())\n",
    "    if len(variables) <= 1:\n",
    "        variables = set()\n",
    "    return variables\n",
    "\n",
    "def read_unconstrained_feature_variables(extractor, revision, architecture):\n",
    "    unconstrained_features_filename = f'{output_directory}/unconstrained-features/{extractor}/linux/{revision}[{architecture}].unconstrained.features'\n",
    "    unconstrained_feature_variables = set()\n",
    "    if os.path.isfile(unconstrained_features_filename):\n",
    "        with open(unconstrained_features_filename, 'r') as f:\n",
    "            unconstrained_feature_variables = set([re.sub('^CONFIG_', '', f.strip()) for f in f.readlines()])\n",
    "    return unconstrained_feature_variables\n",
    "\n",
    "def inspect_architecture_features_for_model(extractor, revision, architecture, config_features, features_for_last_revision):\n",
    "    global potential_misses_grep, potential_misses_kmax\n",
    "    \n",
    "    features_filename = f'{output_directory}/kconfig/{extractor}/linux/{revision}[{architecture}].features'\n",
    "    with open(features_filename, 'r') as f:\n",
    "        extracted_features = set([re.sub('^CONFIG_', '', f.strip()) for f in f.readlines()])\n",
    "    \n",
    "    unconstrained_feature_variables = read_unconstrained_feature_variables(extractor, revision, architecture)\n",
    "\n",
    "    dimacs_filename = f'{output_directory}/backbone-dimacs/{extractor}/linux/{revision}[{architecture}].backbone.dimacs'\n",
    "    all_variables = set()\n",
    "    variables = set()\n",
    "    feature_variables = set()\n",
    "    core_feature_variables = set()\n",
    "    dead_feature_variables = set()\n",
    "    undead_feature_variables = set()\n",
    "    all_feature_variables = set()\n",
    "    features = set()\n",
    "    core_features = set()\n",
    "    unconstrained_features = set()\n",
    "    constrained_features = set()\n",
    "    added_features = None\n",
    "    removed_features = None\n",
    "    infos = {'extracted_features_jaccard': np.nan, \\\n",
    "                    'all_variables_jaccard': np.nan, \\\n",
    "                    'variables_jaccard': np.nan, \\\n",
    "                    'feature_variables_jaccard': np.nan, \\\n",
    "                    'undead_feature_variables_jaccard': np.nan, \\\n",
    "                    'all_feature_variables_jaccard': np.nan, \\\n",
    "                    'features_jaccard': np.nan, \\\n",
    "                    'unconstrained_bools': np.nan, \\\n",
    "                    'unconstrained_tristates': np.nan}\n",
    "    \n",
    "    if os.path.isfile(dimacs_filename):\n",
    "        with open(dimacs_filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            all_variable_map = {}\n",
    "            variable_map = {}\n",
    "            feature_variable_map = {}\n",
    "            for f in lines:\n",
    "                if f.startswith('c '):\n",
    "                    result = re.search('^c ([^ ]+) ([^ ]+)$', f)\n",
    "                    if result:\n",
    "                        index = int(result.group(1).strip())\n",
    "                        name = result.group(2).strip()\n",
    "                        all_variable_map[index] = name\n",
    "                        if \"k!\" not in name:\n",
    "                            variable_map[index] = name\n",
    "                            if name != 'True' \\\n",
    "                                and name != '<unsupported>' \\\n",
    "                                and name != 'PREDICATE_Compare' \\\n",
    "                                and not name.startswith('__VISIBILITY__CONFIG_') \\\n",
    "                                and not name.endswith('_MODULE'):\n",
    "                                feature_variable_map[index] = name\n",
    "            all_variables = get_variables(all_variable_map)\n",
    "            variables = get_variables(variable_map)\n",
    "            feature_variables = get_variables(feature_variable_map)\n",
    "\n",
    "            backbone_features_filename = f'{output_directory}/backbone-features/{extractor}/linux/{revision}[{architecture}].backbone.features'\n",
    "            if os.path.isfile(backbone_features_filename):\n",
    "                with open(backbone_features_filename, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    if len(lines) > 1:\n",
    "                        core_feature_variables = set([line[1:].strip() for line in lines if line.startswith('+')]).intersection(feature_variables)\n",
    "                        dead_feature_variables = set([line[1:].strip() for line in lines if line.startswith('-')]).intersection(feature_variables)\n",
    "\n",
    "            if len(feature_variables) > 0:\n",
    "                undead_feature_variables = feature_variables.difference(dead_feature_variables)\n",
    "                all_feature_variables = undead_feature_variables.union(unconstrained_feature_variables)\n",
    "                features = all_feature_variables.intersection(config_features)\n",
    "                if f'{revision}###{architecture}' not in extractor_comparison:\n",
    "                    extractor_comparison[f'{revision}###{architecture}'] = features\n",
    "                else:\n",
    "                    extractor_comparison[f'{revision}###{architecture}'] = jaccard(extractor_comparison[f'{revision}###{architecture}'], features)\n",
    "                core_features = features.intersection(core_feature_variables)\n",
    "                unconstrained_features = features.intersection(unconstrained_feature_variables)\n",
    "                unconstrained_features_by_type = pd.DataFrame(list(unconstrained_features), columns=['config']) \\\n",
    "                    .merge(df_config_types[(df_config_types['revision'] == revision)])\n",
    "                unconstrained_bools = unconstrained_features_by_type[unconstrained_features_by_type['type'] == 'bool']['config'].drop_duplicates()\n",
    "                unconstrained_tristates = unconstrained_features_by_type[unconstrained_features_by_type['type'] == 'tristate']['config'].drop_duplicates()\n",
    "                constrained_features = features.difference(core_feature_variables).difference(unconstrained_feature_variables)\n",
    "                if architecture in features_for_last_revision and len(features_for_last_revision[architecture]) > 0:\n",
    "                    added_features = features.difference(features_for_last_revision[architecture])\n",
    "                    removed_features = features_for_last_revision[architecture].difference(features)\n",
    "                infos = { \\\n",
    "                            'extracted_features_jaccard': jaccard(extracted_features, features), \\\n",
    "                            'all_variables_jaccard': jaccard(all_variables, features), \\\n",
    "                            'variables_jaccard': jaccard(variables, features), \\\n",
    "                            'feature_variables_jaccard': jaccard(feature_variables, features), \\\n",
    "                            'undead_feature_variables_jaccard': jaccard(undead_feature_variables, features), \\\n",
    "                            'all_feature_variables_jaccard': jaccard(all_feature_variables, features), \\\n",
    "                            'features_jaccard': 1, \\\n",
    "                            'unconstrained_bools': len(unconstrained_bools), \\\n",
    "                            'unconstrained_tristates': len(unconstrained_tristates) \\\n",
    "                        }\n",
    "    descriptor = {'extractor': extractor, 'revision': revision, 'architecture': architecture} | infos\n",
    "    add_features(descriptor, 'config_features', config_features) # F_config\n",
    "    add_features(descriptor, 'extracted_features', extracted_features) # F_extracted\n",
    "    add_features(descriptor, 'unconstrained_feature_variables', unconstrained_feature_variables, min=1) # F_unconstrained\n",
    "    add_features(descriptor, 'all_variables', all_variables) # V_all\n",
    "    add_features(descriptor, 'variables', variables) # V_phi\n",
    "    add_features(descriptor, 'feature_variables', feature_variables) # FV_phi\n",
    "    add_features(descriptor, 'core_feature_variables', core_feature_variables, min=1) # FV_core\n",
    "    add_features(descriptor, 'dead_feature_variables', dead_feature_variables, min=1) # FV_dead\n",
    "    add_features(descriptor, 'constrained_feature_variables', undead_feature_variables.difference(core_feature_variables)) # FV_constrained\n",
    "    add_features(descriptor, 'undead_feature_variables', undead_feature_variables) # FV_undead\n",
    "    add_features(descriptor, 'all_feature_variables', all_feature_variables) # FV\n",
    "    add_features(descriptor, 'ALL_feature_variables', feature_variables.union(unconstrained_feature_variables)) # FV_all\n",
    "    add_features(descriptor, 'features', features) # F\n",
    "    add_features(descriptor, 'core_features', core_features, min=1)\n",
    "    add_features(descriptor, 'unconstrained_features', unconstrained_features, min=1)\n",
    "    add_features(descriptor, 'constrained_features', constrained_features)\n",
    "    add_features(descriptor, 'added_features', added_features, min=0)\n",
    "    add_features(descriptor, 'removed_features', removed_features, min=0)\n",
    "    if extractor == 'kmax':\n",
    "        potential_misses_grep.update([f for f in all_feature_variables.difference(features) if '__CONFIG_' not in f])\n",
    "    return descriptor, feature_variables.union(unconstrained_feature_variables), features\n",
    "\n",
    "def inspect_architecture_features_for_revision(extractor, revision, features_for_last_revision):\n",
    "    config_features = set(df_configs[df_configs['revision'] == revision]['config'])\n",
    "    architectures = [re.search('\\[(.*)\\]', f).group(1) for f in glob.glob(f'{output_directory}/kconfig/{extractor}/linux/{revision}[*.features')]\n",
    "    architectures = list(set(architectures))\n",
    "    architectures.sort()\n",
    "    data = []\n",
    "    total_features = set()\n",
    "    total_feature_variables = set()\n",
    "    features_for_current_revision = {}\n",
    "    for architecture in architectures:\n",
    "        descriptor, feature_variables, features = inspect_architecture_features_for_model(extractor, revision, architecture, config_features, features_for_last_revision)\n",
    "        data.append(descriptor)\n",
    "        total_features.update(features)\n",
    "        features_for_current_revision[architecture] = features\n",
    "        if extractor == 'kmax':\n",
    "            total_feature_variables.update(feature_variables)\n",
    "    for descriptor in data:\n",
    "        add_features(descriptor, 'total_features', total_features)\n",
    "        total_added_features = None\n",
    "        total_removed_features = None\n",
    "        if 'TOTAL' in features_for_last_revision and len(features_for_last_revision['TOTAL']) > 0:\n",
    "            total_added_features = total_features.difference(features_for_last_revision['TOTAL'])\n",
    "            total_removed_features = features_for_last_revision['TOTAL'].difference(total_features)\n",
    "        add_features(descriptor, 'total_added_features', total_added_features, min=0)\n",
    "        add_features(descriptor, 'total_removed_features', total_removed_features, min=0)\n",
    "    features_for_current_revision['TOTAL'] = total_features\n",
    "    df_configs_configurable.loc[(df_configs_configurable['revision'] == revision) & (df_configs_configurable['config'].isin(total_features)), 'configurable'] = True\n",
    "    if extractor == 'kmax':\n",
    "        potential_misses_kmax.update([f for f in config_features.difference(total_feature_variables)])\n",
    "    return data, features_for_current_revision\n",
    "\n",
    "def inspect_architecture_features(extractor):\n",
    "    print(f'{extractor} ', end='')\n",
    "    revisions = [re.search('/linux/(.*)\\[', f).group(1) for f in glob.glob(f'{output_directory}/kconfig/{extractor}/linux/*.features')]\n",
    "    revisions = list(set(revisions))\n",
    "    revisions.sort(key=Version)\n",
    "    data = []\n",
    "    features_for_last_revision = {}\n",
    "    i = 0\n",
    "    for revision in revisions:\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            print(revision + ' . ', end='')\n",
    "        new_data, features_for_last_revision = inspect_architecture_features_for_revision(extractor, revision, features_for_last_revision)\n",
    "        data += new_data\n",
    "    print()\n",
    "    return data\n",
    "\n",
    "if os.path.isfile(f'{output_directory}/linux-features.dat'):\n",
    "    with open(f'{output_directory}/linux-features.dat', 'rb') as f:\n",
    "        [features_by_kind_per_architecture, df_extractor_comparison, potential_misses_grep, potential_misses_kmax, df_configs_configurable] = pickle.load(f)\n",
    "else:\n",
    "    features_by_kind_per_architecture = inspect_architecture_features('kconfigreader')\n",
    "    features_by_kind_per_architecture += inspect_architecture_features('kmax')\n",
    "    features_by_kind_per_architecture = pd.DataFrame(features_by_kind_per_architecture)\n",
    "    df_extractor_comparison = []\n",
    "    for key, value in extractor_comparison.items():\n",
    "        [revision, architecture] = key.split('###')\n",
    "        if type(value) is set:\n",
    "            value = pd.NA\n",
    "        df_extractor_comparison.append({'revision': revision, 'architecture': architecture, 'extractor_jaccard': value})\n",
    "    df_extractor_comparison = pd.DataFrame(df_extractor_comparison)\n",
    "    with open(f'{output_directory}/linux-features.dat', 'wb') as f:\n",
    "        pickle.dump([features_by_kind_per_architecture, df_extractor_comparison, potential_misses_grep, potential_misses_kmax, df_configs_configurable], f)\n",
    "\n",
    "replace_values(features_by_kind_per_architecture)\n",
    "df_features = pd.merge(df_architectures, features_by_kind_per_architecture, how='outer').sort_values(by='committer_date')\n",
    "df_features = pd.merge(df_kconfig, df_features, how='outer').sort_values(by='committer_date')\n",
    "\n",
    "def compare_with_grep(message, list):\n",
    "    print(f'{message}: ' + str(len(list)))\n",
    "    print(pd.merge(df_configs[['config','kconfig-file']], pd.DataFrame(list, columns=['config']), how='inner') \\\n",
    "        .drop_duplicates().merge(df_config_types[['config', 'type']]).drop_duplicates())\n",
    "\n",
    "def report_potential_misses(potential_misses_grep, potential_misses_kmax):\n",
    "    # these are the features NOT found by grep, but found by kmax (this allows us to check whether the grep regex matches too much)\n",
    "    # the only matches are enviroment variables (e.g., ARCH) and mistakes in kconfig files: IA64_SGI_UV (which has a trailing `) and SND_SOC_UX500_MACH_MOP500 (which has a leading +)\n",
    "    compare_with_grep('#potential misses (grep)', potential_misses_grep)\n",
    "    print()\n",
    "\n",
    "    # these are the features found by grep, but NOT found by kmax, either constrained or unconstrained (this allows us to check whether kmax matches enough)\n",
    "    # as there are some extraction failures for kmax, we expect some misses; also, we do not extract the um architecture; and finally, there are some test kconfig files that are never included\n",
    "    # in the following, we try to filter out these effects (this is not perfect though)\n",
    "    potential_misses_kmax_with_type = (pd.merge(df_configs[['config','kconfig-file', 'revision']], pd.DataFrame(potential_misses_kmax, columns=['config']), how='inner') \\\n",
    "            .drop_duplicates().merge(df_config_types[['config', 'type']]).drop_duplicates())\n",
    "    misses_due_to_tests = set(potential_misses_kmax_with_type[ \\\n",
    "            potential_misses_kmax_with_type['kconfig-file'].str.startswith('Documentation/') | \\\n",
    "            potential_misses_kmax_with_type['kconfig-file'].str.startswith('scripts/')]['config'].unique())\n",
    "    missing_kmax_models = df_features[(df_features['extractor'] == 'KClause') & df_features['#extracted_features'].isna()]\n",
    "    missing_kmax_models = missing_kmax_models[['revision', 'architecture']].drop_duplicates()\n",
    "    potential_misses_kmax_with_type['architecture'] = potential_misses_kmax_with_type['kconfig-file'].apply(lambda s: re.sub(r'^arch/(.*?)/.*$', r'\\1', s))\n",
    "    potential_misses_due_to_missing_kmax_models = set(potential_misses_kmax_with_type.merge(missing_kmax_models[['revision', 'architecture']].drop_duplicates()) \\\n",
    "                                                    .drop(columns=['kconfig-file', 'revision', 'architecture', 'type'])['config'].unique())\n",
    "    potential_misses_kmax = potential_misses_kmax.difference(misses_due_to_tests).difference(potential_misses_due_to_missing_kmax_models)\n",
    "    # the remaining matches are due to our way of using kmax extractor, where we ignore lines with new kconfig constructs like $(success,...)\n",
    "    compare_with_grep('#potential misses (kmax)', potential_misses_kmax)\n",
    "\n",
    "report_potential_misses(potential_misses_grep, potential_misses_kmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alpha', 'arm', 'cris', 'i386', 'ia64', 'm68k', 'mips', 'mips64',\n",
       "       'parisc', 'ppc', 'ppc64', 's390', 's390x', 'sh', 'sparc',\n",
       "       'sparc64', 'x86_64', 'm68knommu', 'v850', 'h8300', 'arm26', 'm32r',\n",
       "       'sh64', 'frv', 'xtensa', 'powerpc', 'avr32', 'blackfin', 'x86',\n",
       "       'mn10300', 'microblaze', 'score', 'tile', 'unicore32', 'openrisc',\n",
       "       'arc', 'arm64', 'c6x', 'hexagon', 'metag', 'nios2', 'riscv',\n",
       "       'nds32', 'csky', 'loongarch'], dtype=object)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kconfig[\"architecture\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.isfile(f'{output_directory}/linux-features.dat'):\n",
    "    with open(f'{output_directory}/linux-features.dat', 'rb') as f:\n",
    "        [features_by_kind_per_architecture, df_extractor_comparison, potential_misses_grep, potential_misses_kmax, df_configs_configurable] = pickle.load(f)\n",
    "else:\n",
    "    features_by_kind_per_architecture = inspect_architecture_features('kconfigreader')\n",
    "    features_by_kind_per_architecture += inspect_architecture_features('kmax')\n",
    "    features_by_kind_per_architecture = pd.DataFrame(features_by_kind_per_architecture)\n",
    "    df_extractor_comparison = []\n",
    "    for key, value in extractor_comparison.items():\n",
    "        [revision, architecture] = key.split('###')\n",
    "        if type(value) is set:\n",
    "            value = pd.NA\n",
    "        df_extractor_comparison.append({'revision': revision, 'architecture': architecture, 'extractor_jaccard': value})\n",
    "    df_extractor_comparison = pd.DataFrame(df_extractor_comparison)\n",
    "df_features = pd.merge(df_architectures, features_by_kind_per_architecture, how='outer').sort_values(by='committer_date')\n",
    "df_features = pd.merge(df_kconfig, df_features, how='outer').sort_values(by='committer_date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linux_total_features():\n",
    "    df_total_features = df_features.groupby(['extractor', 'revision']).agg({'#total_features': 'min'}).reset_index()\n",
    "    df_total_features = pd.merge(df_kconfig[['committer_date', 'revision']].drop_duplicates(), df_total_features)\n",
    "    return df_total_features.sort_values(by=[\"committer_date\"]).tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287    19812.0\n",
       "269    19898.0\n",
       "268    19740.0\n",
       "270    19869.0\n",
       "271    20024.0\n",
       "Name: #total_features, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linux_total_features()[\"#total_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_for(df, column, committer_date):\n",
    "    x = df.sort_values(by=[committer_date])\n",
    "    return x.tail(1)[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_revision(df):\n",
    "    x = df[df['revision'].str.contains(\"\\w\\d+\\.0$\", regex=True)]\n",
    "    if len(x) == 0:\n",
    "        x = df.sort_values(by=[\"revision\"])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_revision(df, revision):\n",
    "    x = df[df['revision'].str.contains(revision, regex=False)]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_arch(df, arch):\n",
    "    return df[df['architecture'] == arch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32      v2.5.45\n",
       "33      v2.5.45\n",
       "70      v2.5.46\n",
       "71      v2.5.46\n",
       "108     v2.5.47\n",
       "         ...   \n",
       "2841     v2.6.7\n",
       "2880     v2.6.8\n",
       "2881     v2.6.8\n",
       "2922     v2.6.9\n",
       "2923     v2.6.9\n",
       "Name: revision, Length: 110, dtype: object"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = for_arch(df_kconfig, \"x86_64\")\n",
    "x = by_revision(x)\n",
    "x[\"revision\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def write_object_to_file(obj, name):\n",
    "    with open(name, 'w') as fp:\n",
    "        json.dump(obj, fp)\n",
    "def read_json(path):\n",
    "    with open(path) as json_data:\n",
    "        return json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_sloc_linux():\n",
    "    output_directory = \"output-linux\"\n",
    "    df_kconfig = read_dataframe(\"kconfig\", output_dir=output_directory, )\n",
    "    archs = df_kconfig[\"architecture\"].unique()\n",
    "    vals = dict()\n",
    "    for arch in archs:\n",
    "        df_arch = for_arch(df_kconfig, arch)\n",
    "        df_arch = by_revision(df_arch)\n",
    "        sloc = int(\n",
    "            latest_for(df_arch, \"source_lines_of_code\", \"committer_date_unix\").iloc[0]\n",
    "        )\n",
    "        last_rev = latest_for(df_arch, \"revision\", \"committer_date_unix\").iloc[0]\n",
    "        major = int(last_rev[1])\n",
    "        before_last = df_arch[df_arch['revision'].str.contains(f\"\\w{major-1}\\.\\d$\", regex=True)]\n",
    "        if len(before_last) == 0:\n",
    "            vals[f\"linux/{arch}\"] = {\n",
    "                \"source_lines_of_code\": {\n",
    "                    \"currentValue\": sloc,\n",
    "                    \"cmpLastRevision\": \"+100% (No Prior Revision)\",\n",
    "                }\n",
    "            }\n",
    "            continue\n",
    "        before_last = before_last[\"source_lines_of_code\"]\n",
    "        before_last = int(before_last.iloc[0])\n",
    "        value = round(100 * (sloc - before_last) / before_last, 2)\n",
    "        vals[f\"linux/{arch}\"] = {\n",
    "            \"source_lines_of_code\": {\n",
    "                \"currentValue\": f\"{sloc} loc\",\n",
    "                \"cmpLastRevision\": f\"{value:+.1f}%\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_metrics(new):\n",
    "    old = read_json(\"src/public/init.json\")\n",
    "    \n",
    "    for proj, metrics in new.items():\n",
    "        for metric, values in metrics.items():\n",
    "            # print(f\"{proj=}, {metric=}, {values=}\")\n",
    "            for name, value in values.items():\n",
    "                if proj not in old[\"projectData\"]:\n",
    "                    print(f\"{proj} not in old\")\n",
    "                    continue\n",
    "                old[\"projectData\"][proj][metric][name] = value\n",
    "    write_object_to_file(old, \"src/public/init.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = get_metrics_sloc_linux()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_metrics(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"output-busybox\"\n",
    "df_kconfig = read_dataframe('kconfig')\n",
    "df_kconfig = df_kconfig[df_kconfig[\"system\"] ==\"busybox\"]\n",
    "def get_metrics_sloc_nonLinux(project):\n",
    "    vals = dict()\n",
    "    df_arch = by_revision(df_kconfig)\n",
    "    lastTwo = df_arch.sort_values(by=\"committer_date_unix\").tail(2)[\"revision\"]\n",
    "    print(lastTwo)\n",
    "    last_rev = lastTwo.iloc[1]\n",
    "    before_last_rev = lastTwo.iloc[0]\n",
    "    sloc = int(df_arch[df_arch[\"revision\"]==last_rev][\"source_lines_of_code\"].iloc[0])\n",
    "    before_last = int(df_arch[df_arch[\"revision\"]==before_last_rev][\"source_lines_of_code\"].iloc[0])\n",
    "    print(sloc, before_last)\n",
    "    value = round(100 * (sloc - before_last) / before_last, 2)\n",
    "    vals[project] = {\n",
    "        \"source_lines_of_code\": {\n",
    "            \"currentValue\": f\"{sloc} loc\",\n",
    "            \"cmpLastRevision\": f\"{value:+.1f}%\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220    1_35_0\n",
      "221    1_36_0\n",
      "Name: revision, dtype: object\n",
      "209492 205741\n"
     ]
    }
   ],
   "source": [
    "x = get_metrics_sloc_nonLinux(\"busybox\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merge_metrics(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
