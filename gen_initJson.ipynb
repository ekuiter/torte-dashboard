{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# pip install plotly pandas statsmodels kaleido scipy nbformat jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import json\n",
    "import pickle\n",
    "import scipy\n",
    "from statistics import mean, stdev\n",
    "from math import sqrt, log10\n",
    "from packaging.version import Version\n",
    "\n",
    "init_json_path = \"vuetify-project/public/init.json\"\n",
    "linux_extractors = [\"KConfigReader\", \"KClause\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(\n",
    "    stage, dtype={}, usecols=None, file=None, output_directory=\"output-linux\"\n",
    "):\n",
    "    if not file:\n",
    "        file = \"output\"\n",
    "    df = pd.read_csv(\n",
    "        f\"{output_directory}/{stage}/{file}.csv\", dtype=dtype, usecols=usecols\n",
    "    )\n",
    "    if \"committer_date_unix\" in df:\n",
    "        df[\"committer_date\"] = df[\"committer_date_unix\"].apply(\n",
    "            lambda d: pd.to_datetime(d, unit=\"s\")\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for drawing plots\n",
    "\n",
    "\n",
    "def estimate_group(group):\n",
    "    print(\"\\\\hspace{2mm} \" + group + \" \\\\\\\\\")\n",
    "\n",
    "\n",
    "def estimate_trend(\n",
    "    fig, color=None, color_value=None, xs=[], key=lambda x: x.timestamp()\n",
    "):\n",
    "    results = px.get_trendline_results(fig)\n",
    "    if color is not None and color_value is not None:\n",
    "        idx = [i for i, r in enumerate(results.iloc) if r[color] == color_value]\n",
    "        if idx != []:\n",
    "            idx = idx[0]\n",
    "        else:\n",
    "            idx = 0\n",
    "    else:\n",
    "        idx = 0\n",
    "    intercept = results.iloc[idx][\"px_fit_results\"].params[0]\n",
    "    slope = results.iloc[idx][\"px_fit_results\"].params[1]\n",
    "    daily = slope * pd.to_timedelta(1, unit=\"D\").total_seconds()\n",
    "    weekly = slope * pd.to_timedelta(7, unit=\"D\").total_seconds()\n",
    "    monthly = slope * pd.to_timedelta(1, unit=\"D\").total_seconds() * 30.437\n",
    "    yearly = slope * pd.to_timedelta(1, unit=\"D\").total_seconds() * 365.25\n",
    "    return daily, weekly, monthly, yearly, [intercept + slope * key(x) for x in xs]\n",
    "\n",
    "\n",
    "def log10_y_axis(fig):\n",
    "    fig.update_yaxes(tickprefix=\"10<sup>\", ticksuffix=\"</sup>\")\n",
    "\n",
    "\n",
    "def percentage_y_axis(fig):\n",
    "    fig.layout.yaxis.tickformat = \",.0%\"\n",
    "\n",
    "\n",
    "def format_percentage(value):\n",
    "    return str(round(value * 100, 2)) + \"%\"\n",
    "\n",
    "\n",
    "def committer_date_labels(dict={}):\n",
    "    return {\"committer_date\": \"Year<br><sup>First Release in Year</sup>\"} | dict\n",
    "\n",
    "\n",
    "def revision_labels(dict={}):\n",
    "    return {\"revision\": \"Year\"} | dict\n",
    "\n",
    "\n",
    "def style_legend(fig, position=\"topleft\", xshift=0, yshift=0):\n",
    "    if position == \"topleft\":\n",
    "        fig.update_layout(\n",
    "            legend=dict(yanchor=\"top\", y=0.98 + yshift, xanchor=\"left\", x=0.01 + xshift)\n",
    "        )\n",
    "    elif position == \"topright\":\n",
    "        fig.update_layout(\n",
    "            legend=dict(\n",
    "                yanchor=\"top\", y=0.98 + yshift, xanchor=\"right\", x=0.98 + xshift\n",
    "            )\n",
    "        )\n",
    "    elif position == \"bottomright\":\n",
    "        fig.update_layout(\n",
    "            legend=dict(\n",
    "                yanchor=\"bottom\", y=0.01 + yshift, xanchor=\"right\", x=0.98 + xshift\n",
    "            )\n",
    "        )\n",
    "    elif position == \"bottomleft\":\n",
    "        fig.update_layout(\n",
    "            legend=dict(\n",
    "                yanchor=\"bottom\", y=0.01 + yshift, xanchor=\"left\", x=0.01 + xshift\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        fig.update_layout(showlegend=False)\n",
    "\n",
    "\n",
    "def style_box(fig, legend_position=\"topleft\", xshift=0, yshift=0):\n",
    "    fig.update_traces(fillcolor=\"rgba(0,0,0,0)\")\n",
    "    fig.update_traces(line_width=1)\n",
    "    fig.update_traces(marker_size=2)\n",
    "    fig.update_layout(font_family=\"Linux Biolinum\")\n",
    "    style_legend(fig, legend_position, xshift, yshift)\n",
    "\n",
    "\n",
    "def style_scatter(fig, marker_size=4, legend_position=\"topleft\", xshift=0, yshift=0):\n",
    "    if marker_size:\n",
    "        fig.update_traces(marker_size=marker_size)\n",
    "    style_legend(fig, legend_position, xshift, yshift)\n",
    "    fig.update_layout(font_family=\"Linux Biolinum\")\n",
    "\n",
    "\n",
    "def plot_failures(\n",
    "    fig, df, x, y, y_value, align=\"bottom\", xref=\"x\", font_size=10, textangle=270\n",
    "):\n",
    "    group = df.groupby(x, dropna=False)\n",
    "    failures = (\n",
    "        (group[y].size() - group[y].count())\n",
    "        .reset_index()\n",
    "        .rename(columns={y: f\"{y}_failures\"})\n",
    "    )\n",
    "    attempts = group[y].size().reset_index().rename(columns={y: f\"{y}_attempts\"})\n",
    "    failures = pd.merge(failures, attempts)\n",
    "    failures[f\"{y}_text\"] = (\n",
    "        failures[f\"{y}_failures\"].astype(str)\n",
    "        + \" (\"\n",
    "        + (failures[f\"{y}_failures\"] / failures[f\"{y}_attempts\"]).apply(\n",
    "            lambda v: \"{0:.1f}%\".format(v * 100)\n",
    "        )\n",
    "        + \")\"\n",
    "    )\n",
    "    for row in range(len(failures)):\n",
    "        text = failures.at[row, f\"{y}_text\"]\n",
    "        text = \"\" if failures.at[row, f\"{y}_failures\"] == 0 else text\n",
    "        fig.add_annotation(\n",
    "            x=failures.at[row, x],\n",
    "            y=y_value,\n",
    "            text=text,\n",
    "            showarrow=False,\n",
    "            font_size=font_size,\n",
    "            textangle=textangle,\n",
    "            align=\"left\" if align == \"bottom\" else \"right\",\n",
    "            yanchor=\"bottom\" if align == \"bottom\" else \"top\",\n",
    "            yshift=5 if align == \"bottom\" else -5,\n",
    "            font_color=\"gray\",\n",
    "            xref=xref,\n",
    "        )\n",
    "\n",
    "\n",
    "def cohens_d(d1, d2):\n",
    "    # uses pooled standard deviation\n",
    "    n1, n2 = len(d1), len(d2)\n",
    "    s1, s2 = np.var(d1, ddof=1), np.var(d2, ddof=1)\n",
    "    s = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    "    u1, u2 = np.mean(d1), np.mean(d2)\n",
    "    return (u1 - u2) / s\n",
    "\n",
    "\n",
    "def wilcoxon_test(df, column_a, column_b):\n",
    "    # if the same values are returned for many inputs, refer to https://stats.stackexchange.com/q/232927\n",
    "    a = df[column_a][~df[column_a].isna()]\n",
    "    b = df[column_b][~df[column_b].isna()]\n",
    "    d = a - b\n",
    "    results = scipy.stats.wilcoxon(d, method=\"approx\")\n",
    "    p = results.pvalue\n",
    "    # adapted from https://stats.stackexchange.com/q/133077\n",
    "    r = np.abs(results.zstatistic / np.sqrt(len(d) * 2))\n",
    "    return p, r\n",
    "\n",
    "\n",
    "def style_p_values(\n",
    "    fig, brackets, scale=0, _format=dict(interline=0.07, text_height=1.07, color=\"gray\")\n",
    "):\n",
    "    # adapted from https://stackoverflow.com/q/67505252\n",
    "    for entry in brackets:\n",
    "        first_column, second_column, y, results = entry\n",
    "        y_range = [1.01 + y * _format[\"interline\"], 1.02 + y * _format[\"interline\"]]\n",
    "        p, r = results\n",
    "        if p >= 0.05:\n",
    "            symbol = \"ns\"\n",
    "        elif p >= 0.01:\n",
    "            symbol = \"*\"\n",
    "        elif p >= 0.001:\n",
    "            symbol = \"**\"\n",
    "        else:\n",
    "            symbol = \"***\"\n",
    "        first_column = first_column - scale\n",
    "        second_column = second_column + scale\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            xref=\"x\",\n",
    "            yref=\"y domain\",\n",
    "            x0=first_column,\n",
    "            y0=y_range[0],\n",
    "            x1=first_column,\n",
    "            y1=y_range[1],\n",
    "            line=dict(\n",
    "                color=_format[\"color\"],\n",
    "                width=2,\n",
    "            ),\n",
    "        )\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            xref=\"x\",\n",
    "            yref=\"y domain\",\n",
    "            x0=first_column,\n",
    "            y0=y_range[1],\n",
    "            x1=second_column,\n",
    "            y1=y_range[1],\n",
    "            line=dict(\n",
    "                color=_format[\"color\"],\n",
    "                width=2,\n",
    "            ),\n",
    "        )\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            xref=\"x\",\n",
    "            yref=\"y domain\",\n",
    "            x0=second_column,\n",
    "            y0=y_range[0],\n",
    "            x1=second_column,\n",
    "            y1=y_range[1],\n",
    "            line=dict(\n",
    "                color=_format[\"color\"],\n",
    "                width=2,\n",
    "            ),\n",
    "        )\n",
    "        fig.add_annotation(\n",
    "            dict(\n",
    "                font=dict(color=_format[\"color\"], size=14),\n",
    "                x=(first_column + second_column) / 2,\n",
    "                y=y_range[1] * _format[\"text_height\"],\n",
    "                showarrow=False,\n",
    "                text=symbol + \" <sup>(\" + str(round(r, 2)) + \")</sup>\",\n",
    "                textangle=0,\n",
    "                xref=\"x\",\n",
    "                yref=\"y domain\",\n",
    "            )\n",
    "        )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def bracket_for(i, j, xshift, y, results):\n",
    "    return [i + xshift, j + xshift, y, results]\n",
    "\n",
    "\n",
    "def filter_extractor(df, extractor):\n",
    "    return df[df[\"extractor\"] == extractor]\n",
    "\n",
    "\n",
    "def annotate_value(\n",
    "    fig,\n",
    "    x,\n",
    "    y,\n",
    "    subplot,\n",
    "    prefix,\n",
    "    ax,\n",
    "    ay,\n",
    "    xanchor,\n",
    "    df,\n",
    "    fn=lambda prefix, y: prefix + \": \" + format(round(y), \",\") if y > 0 else prefix,\n",
    "):\n",
    "    if df.empty:\n",
    "        return\n",
    "    if isinstance(x, str):\n",
    "        x = df[x].iat[0]\n",
    "    if isinstance(y, str):\n",
    "        y = df[y].iat[0]\n",
    "    fig.add_annotation(\n",
    "        xref=\"x\" + str(subplot),\n",
    "        yref=\"y\" + str(subplot),\n",
    "        x=x,\n",
    "        y=y,\n",
    "        ax=ax,\n",
    "        ay=ay,\n",
    "        xanchor=xanchor,\n",
    "        text=fn(prefix, y),\n",
    "    )\n",
    "\n",
    "\n",
    "def show(fig, name=None, width=1000, height=500, margin=None):\n",
    "    # fig.update_layout(width=width, height=height)\n",
    "    if margin:\n",
    "        fig.update_layout(margin=margin)\n",
    "    else:\n",
    "        fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "\n",
    "    # if figures_directory and os.path.isdir(figures_directory) and name:\n",
    "    # fig.write_image(f'{figures_directory}/{name}.pdf')\n",
    "    # fig.write_html(f'{figures_directory}/{name}.html',config={\"responsive\":True})\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_arch(df):\n",
    "    grouped = df.groupby(\"architecture\")\n",
    "    dfs = {arch: group for arch, group in grouped}\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def read_dataframe_linux(stage, dtype={}, usecols=None, file=None, arch=None):\n",
    "    if not file:\n",
    "        file = \"output\"\n",
    "    df = pd.read_csv(f\"output-linux/{stage}/{file}.csv\", dtype=dtype, usecols=usecols)\n",
    "    if \"committer_date_unix\" in df:\n",
    "        df[\"committer_date\"] = df[\"committer_date_unix\"].apply(\n",
    "            lambda d: pd.to_datetime(d, unit=\"s\")\n",
    "        )\n",
    "    if arch != None:\n",
    "        return group_by_arch(df)[arch]\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_values(df):\n",
    "    df.replace(\"kconfigreader\", \"KConfigReader\", inplace=True)\n",
    "    df.replace(\"kmax\", \"KClause\", inplace=True)\n",
    "\n",
    "\n",
    "def big_log10(str):\n",
    "    return log10(int(str)) if not pd.isna(str) and str != \"\" else pd.NA\n",
    "\n",
    "\n",
    "def process_model_count(df_solve):\n",
    "    df_solve[\"model-count\"] = df_solve[\"model-count\"].replace(\"1\", \"\")\n",
    "    df_solve[\"model-count-log10\"] = (\n",
    "        df_solve[\"model-count\"].fillna(\"\").apply(big_log10).replace(0, np.nan)\n",
    "    )\n",
    "    df_solve[\"year\"] = df_solve[\"committer_date\"].apply(lambda d: int(d.year))\n",
    "\n",
    "\n",
    "def peek_dataframe(\n",
    "    df, column, message, type=\"str\", filter=[\"revision\", \"architecture\", \"extractor\"]\n",
    "):\n",
    "    success = df[\n",
    "        ~df[column].str.contains(\"NA\") if type == \"str\" else ~df[column].isna()\n",
    "    ][filter]\n",
    "    failure = df[df[column].str.contains(\"NA\") if type == \"str\" else df[column].isna()][\n",
    "        filter\n",
    "    ]\n",
    "    print(f\"{message}: {len(success)} successes, {len(failure)} failures\")\n",
    "\n",
    "\n",
    "def jaccard(a, b):\n",
    "    return len(set.intersection(a, b)) / len(set.union(a, b))\n",
    "\n",
    "\n",
    "def add_features(descriptor, source, features, min=2):\n",
    "    descriptor[f\"#{source}\"] = (\n",
    "        len(features) if features is not None and len(features) >= min else np.nan\n",
    "    )\n",
    "\n",
    "\n",
    "def get_variables(variable_map):\n",
    "    variables = set(variable_map.values())\n",
    "    if len(variables) <= 1:\n",
    "        variables = set()\n",
    "    return variables\n",
    "def number_of_models(df):\n",
    "    return len(df[['revision','architecture', 'extractor']].drop_duplicates())\n",
    "\n",
    "def unify_solvers(df, columns=['model-count-unconstrained-log10']):\n",
    "    return df[['revision', 'committer_date', 'architecture', 'extractor', *columns]].drop_duplicates()\n",
    "\n",
    "def is_accurate(series):\n",
    "    return len(set.difference(set(series), {pd.NA})) < 2\n",
    "\n",
    "def big_sum(series):\n",
    "    big_sum = sum([int(value) for value in series if not pd.isna(value) and value])\n",
    "    if big_sum > 0:\n",
    "        return len(str(big_sum))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/tmp/ipykernel_15503/588457255.py:7: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  x = df[df[\"revision\"].str.contains(\"\\w\\d+\\.0$\", regex=True)]\n"
     ]
    }
   ],
   "source": [
    "def latest_for(df, column, committer_date):\n",
    "    x = df.sort_values(by=[committer_date])\n",
    "    return x.tail(1)[column]\n",
    "\n",
    "\n",
    "def by_revision(df):\n",
    "    x = df[df[\"revision\"].str.contains(\"\\w\\d+\\.0$\", regex=True)]\n",
    "    if len(x) == 0:\n",
    "        x = df.sort_values(by=[\"revision\"])\n",
    "    return x\n",
    "\n",
    "\n",
    "def find_revision(df, revision):\n",
    "    x = df[df[\"revision\"].str.contains(revision, regex=False)]\n",
    "    return x\n",
    "\n",
    "\n",
    "def for_arch(df, arch):\n",
    "    return df[df[\"architecture\"] == arch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_object_to_file(obj, name):\n",
    "    with open(name, \"w\") as fp:\n",
    "        json.dump(obj, fp)\n",
    "\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path) as json_data:\n",
    "        return json.load(json_data)\n",
    "\n",
    "\n",
    "def merge_metrics(new):\n",
    "    old = read_json(init_json_path)\n",
    "\n",
    "    for proj, metrics in new.items():\n",
    "        for metric, values in metrics.items():\n",
    "            # print(f\"{proj=}, {metric=}, {values=}\")\n",
    "            for name, value in values.items():\n",
    "                if proj not in old[\"projectData\"]:\n",
    "                    print(f\"{proj} not in old\")\n",
    "                    continue\n",
    "                old[\"projectData\"][proj][metric][name] = value\n",
    "    write_object_to_file(old, init_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linux:\n",
    "    def read_dataframe(self, stage, dtype={}, usecols=None, file=None):\n",
    "        if not file:\n",
    "            file = \"output\"\n",
    "        df = pd.read_csv(\n",
    "            f\"{self.output_directory}/{stage}/{file}.csv\", dtype=dtype, usecols=usecols\n",
    "        )\n",
    "        if \"committer_date_unix\" in df:\n",
    "            df[\"committer_date\"] = df[\"committer_date_unix\"].apply(\n",
    "                lambda d: pd.to_datetime(d, unit=\"s\")\n",
    "            )\n",
    "        return df\n",
    "    def solver_successes(self, solver):\n",
    "        df_solve_for_solver = self.df_solve_attempts[~self.df_solve_attempts['model-count'].isna()]\n",
    "        df_solve_for_solver = df_solve_for_solver[df_solve_for_solver['backbone.dimacs-analyzer'] == solver]\n",
    "        return set(df_solve_for_solver['extractor'] + ',' + df_solve_for_solver['revision'] + ',' + df_solve_for_solver['architecture'])\n",
    "\n",
    "    def __init__(self):\n",
    "        self.output_directory = \"output-linux\"\n",
    "        self.df_kconfig = self.read_dataframe(\"kconfig\")\n",
    "        self.df_kconfig[\"year\"] = self.df_kconfig[\"committer_date\"].apply(\n",
    "            lambda d: int(d.year)\n",
    "        )\n",
    "        self.df_architectures = self.read_dataframe(\"read-linux-architectures\")\n",
    "        self.df_architectures = self.df_architectures.sort_values(by=\"committer_date\")\n",
    "        self.df_architectures[\"year\"] = self.df_architectures[\"committer_date\"].apply(\n",
    "            lambda d: int(d.year)\n",
    "        )\n",
    "        self.df_configs = self.read_dataframe(\"read-linux-configs\")\n",
    "        self.df_configs = self.df_configs[\n",
    "            ~self.df_configs[\"kconfig-file\"].str.contains(\"/um/\")\n",
    "        ]\n",
    "        self.df_config_types = self.read_dataframe(\n",
    "            \"read-linux-configs\", file=\"output.types\"\n",
    "        )\n",
    "        self.df_config_types = self.df_config_types[\n",
    "            ~self.df_config_types[\"kconfig-file\"].str.contains(\"/um/\")\n",
    "        ]\n",
    "        self.df_config_types = self.df_config_types.merge(\n",
    "            self.df_architectures[[\"revision\", \"committer_date\"]].drop_duplicates()\n",
    "        )\n",
    "        self.df_uvl = self.read_dataframe(\"model_to_uvl_featureide\")\n",
    "        self.df_smt = self.read_dataframe(\"model_to_smt_z3\")\n",
    "        self.df_dimacs = self.read_dataframe(\"dimacs\")\n",
    "        self.df_backbone_dimacs = self.read_dataframe(\"backbone-dimacs\")\n",
    "        self.df_solve = self.read_dataframe(\n",
    "            \"solve_model-count\", {\"model-count\": \"string\"}\n",
    "        )\n",
    "        process_model_count(self.df_solve)\n",
    "        if os.path.isfile(f'{self.output_directory}/model-count-with-6h-timeout.csv'):\n",
    "            self.df_solve_6h = pd.read_csv(f'{self.output_directory}/model-count-with-6h-timeout.csv', dtype={'model-count': 'string'})\n",
    "            self.df_solve_6h = self.df_backbone_dimacs.merge(self.df_solve_6h)\n",
    "            process_model_count(self.df_solve_6h)\n",
    "            self.df_solve = pd.merge(self.df_solve, self.df_solve_6h[['revision','architecture', 'extractor', 'backbone.dimacs-analyzer']], indicator=True, how='outer') \\\n",
    "                .query('_merge==\"left_only\"') \\\n",
    "                .drop('_merge', axis=1)\n",
    "            self.df_solve = pd.concat([self.df_solve, self.df_solve_6h])\n",
    "        else:\n",
    "            self.df_solve_6h = None\n",
    "        for df in [\n",
    "            self.df_kconfig,\n",
    "            self.df_uvl,\n",
    "            self.df_smt,\n",
    "            self.df_dimacs,\n",
    "            self.df_backbone_dimacs,\n",
    "            self.df_solve,\n",
    "        ]:\n",
    "            df.replace(\"kconfigreader\", \"KConfigReader\", inplace=True)\n",
    "            df.replace(\"kmax\", \"KClause\", inplace=True)\n",
    "        self.df_configs_configurable = self.df_configs.copy()\n",
    "        self.df_configs_configurable[\"configurable\"] = False\n",
    "        with open(f\"{self.output_directory}/linux-features.dat\", \"rb\") as f:\n",
    "            [\n",
    "                self.features_by_kind_per_architecture,\n",
    "                self.df_extractor_comparison,\n",
    "                self.potential_misses_grep,\n",
    "                self.potential_misses_kmax,\n",
    "                self.df_configs_configurable,\n",
    "            ] = pickle.load(f)\n",
    "\n",
    "        replace_values(self.features_by_kind_per_architecture)\n",
    "        self.df_features = pd.merge(\n",
    "            self.df_architectures, self.features_by_kind_per_architecture, how=\"outer\"\n",
    "        ).sort_values(by=\"committer_date\")\n",
    "        self.df_features = pd.merge(\n",
    "            self.df_kconfig, self.df_features, how=\"outer\"\n",
    "        ).sort_values(by=\"committer_date\")\n",
    "        self.df_total_features = (\n",
    "            self.df_features.groupby([\"extractor\", \"revision\"])\n",
    "            .agg({\"#total_features\": \"min\"})\n",
    "            .reset_index()\n",
    "        )\n",
    "        self.df_total_features = pd.merge(\n",
    "            self.df_kconfig[[\"committer_date\", \"revision\"]].drop_duplicates(),\n",
    "            self.df_total_features,\n",
    "        )\n",
    "        df_solve_unconstrained = self.df_solve.merge(self.df_features)\n",
    "        df_solve_unconstrained[\"model-count-unconstrained\"] = df_solve_unconstrained.apply(\n",
    "            lambda row: str(\n",
    "                int(row[\"model-count\"])\n",
    "                * (2 ** int(row[\"unconstrained_bools\"]))\n",
    "                * (3 ** int(row[\"unconstrained_tristates\"]))\n",
    "            )\n",
    "            if not pd.isna(row[\"model-count\"]) and row[\"model-count\"] != \"\"\n",
    "            else pd.NA,\n",
    "            axis=1,\n",
    "        )\n",
    "        df_solve_unconstrained[\"model-count-unconstrained-log10\"] = (\n",
    "            df_solve_unconstrained[\"model-count-unconstrained\"]\n",
    "            .fillna(\"\")\n",
    "            .map(big_log10)\n",
    "            .replace(0, np.nan)\n",
    "        )\n",
    "        df_solve_unconstrained[\"similarity\"] = df_solve_unconstrained.apply(\n",
    "            lambda row: int(row[\"model-count\"]) / int(row[\"model-count-unconstrained\"])\n",
    "            if not pd.isna(row[\"model-count\"]) and row[\"model-count\"] != \"\"\n",
    "            else pd.NA,\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        def unify_solvers(df, columns=['model-count-unconstrained-log10']):\n",
    "            return df[['revision', 'committer_date', 'architecture', 'extractor', *columns]].drop_duplicates()\n",
    "\n",
    "        def big_sum(series):\n",
    "            big_sum = sum([int(value) for value in series if not pd.isna(value) and value])\n",
    "            if big_sum > 0:\n",
    "                return len(str(big_sum))\n",
    "            \n",
    "        self.df_solve_slice = df_solve_unconstrained[df_solve_unconstrained['year'] <= 2013]\n",
    "        self.df_solve_failures = self.df_solve_slice.groupby(['extractor', 'revision', 'architecture'], dropna=False).agg({'model-count-unconstrained-log10': lambda x: (True in list(pd.notna(x)) or pd.NA)}).reset_index()\n",
    "        self.df_solve_group = self.df_solve_failures.groupby(['extractor', 'revision'], dropna=False)\n",
    "        self.df_solve_failures = (self.df_solve_group['model-count-unconstrained-log10'].size() - self.df_solve_group['model-count-unconstrained-log10'].count()).reset_index()\n",
    "        self.df_solve_failures['is-upper-bound'] = self.df_solve_failures['model-count-unconstrained-log10'] == 0\n",
    "        self.df_solve_failures = self.df_solve_failures.rename(columns={'model-count-unconstrained-log10': 'failures'})\n",
    "        self.df_solve_total = unify_solvers(pd.merge(self.df_solve_slice, self.df_solve_failures), ['model-count-unconstrained', 'model-count-unconstrained-log10', 'is-upper-bound', 'failures', 'year'])\n",
    "        self.df_solve_total = self.df_solve_total.groupby(['extractor', 'committer_date', 'year']).agg({'model-count-unconstrained': big_sum, 'is-upper-bound': 'min', 'failures': 'min'}).reset_index()\n",
    "    \n",
    "        self.metrics = {f\"linux/{arch}\": dict() for arch in self.df_kconfig[\"architecture\"].unique()}\n",
    "        self.metrics[\"linux/all\"] = dict()\n",
    "\n",
    "    def differentiate_extractors(self, df, sortBy, key, prefix, unit, apply_func=None):\n",
    "        \"\"\"returns: {\n",
    "            \"extractor1\": {\n",
    "                \"value\": \"<prefix><value1> <unit>\",\n",
    "                \"date\": \"From <date1>\"},\n",
    "            \"extractor2\": {\n",
    "                \"value\": \"<prefix><value2> <unit>\",\n",
    "                \"date\": \"From <date2>\"\n",
    "            }\n",
    "        \"\"\"\n",
    "        vals = {\"currentValue\": dict()}\n",
    "        for extractor in linux_extractors:\n",
    "            ex_value = df[df[\"extractor\"] == extractor].dropna().sort_values(sortBy).tail(1)\n",
    "            date = \"Date not Found\"\n",
    "            date_prefix = \"\"\n",
    "            if not ex_value.empty:\n",
    "                date = ex_value[\"committer_date\"].dt.strftime(\"%B %d, %Y\").iloc[0]\n",
    "                date_prefix = \"From\"\n",
    "                if apply_func:\n",
    "                    ex_value = apply_func(ex_value.iloc[0][key])\n",
    "            else:\n",
    "                ex_value = 0\n",
    "                prefix = \"\"\n",
    "                \n",
    "            vals[\"currentValue\"][extractor] = {\n",
    "                \"value\": f\"{prefix}{ex_value} {unit}\",\n",
    "                \"date\": f\"{date_prefix} {date}\"\n",
    "            }\n",
    "        return vals\n",
    "\n",
    "    def model_count_time_latest(self):\n",
    "        archs = list(self.df_kconfig[\"architecture\"].unique())\n",
    "        for arch in archs:\n",
    "            df_arch = self.df_solve_slice[self.df_solve_slice[\"architecture\"]==arch]\n",
    "            key = \"backbone.dimacs-analyzer-time\"\n",
    "            extractor_values = self.differentiate_extractors(df=df_arch, sortBy=\"committer_date_unix\", key=key, prefix=\"10^\", unit=\"s\", apply_func=lambda v: int(v)//1000000000)\n",
    "            self.metrics[f\"linux/{arch}\"][\"model-count-time\"]= extractor_values\n",
    "\n",
    "    def model_count_latest(self):\n",
    "        archs = list(self.df_kconfig[\"architecture\"].unique())\n",
    "        archs.append(\"all\")\n",
    "        for arch in archs:\n",
    "            df_arch = self.df_solve_slice[self.df_solve_slice[\"architecture\"]==arch]\n",
    "            key = \"model-count-unconstrained-log10\"\n",
    "            sortBy = \"committer_date_unix\"\n",
    "            if arch == 'all':\n",
    "                df_arch = self.df_solve_total\n",
    "                key = \"model-count-unconstrained\"\n",
    "                sortBy = \"committer_date\"\n",
    "            extractor_values = self.differentiate_extractors(df=df_arch, sortBy=sortBy, key=key, prefix=\"10^\", unit=\"models\", apply_func=lambda v: int(v))\n",
    "            self.metrics[f\"linux/{arch}\"][\"model-count\"]= extractor_values\n",
    "        \n",
    "    def fill_metrics(self):\n",
    "        self.total_features_latest()\n",
    "        self.features_latest()\n",
    "        self.sloc_latest()\n",
    "        self.model_count_latest()\n",
    "        self.model_count_time_latest()\n",
    "        merge_metrics(self.metrics)\n",
    "\n",
    "    def total_features_latest(self):\n",
    "        extractor_values = self.differentiate_extractors(df=self.df_total_features, sortBy=\"committer_date\", key=\"#total_features\", prefix=\"\", unit=\"features\", apply_func=lambda v: int(v))\n",
    "        self.metrics[\"linux/all\"][\"total-features\"] = extractor_values\n",
    "\n",
    "    def features_latest(self):\n",
    "        archs = list(self.df_kconfig[\"architecture\"].unique())\n",
    "        for architecture in archs:\n",
    "            df = for_arch(self.df_features, architecture)\n",
    "            extractor_values = self.differentiate_extractors(df=df, sortBy=\"committer_date\", key=\"#features\", prefix=\"\", unit=\"features\", apply_func=lambda v: int(v))\n",
    "            self.metrics[f\"linux/{architecture}\"][\"features\"] = extractor_values\n",
    "\n",
    "    def sloc_latest(self):\n",
    "        archs = list(self.df_kconfig[\"architecture\"].unique())\n",
    "        archs.append(\"all\")\n",
    "        key = \"source_lines_of_code\"\n",
    "        for arch in archs:\n",
    "            date = \"Date not Found\"\n",
    "            date_prefix = \"\"\n",
    "            df_arch = for_arch(self.df_kconfig, arch)\n",
    "            if arch == \"all\":\n",
    "                df_arch = self.df_kconfig\n",
    "            value = df_arch.sort_values(by=\"committer_date_unix\").tail(1)\n",
    "            if not value.empty:\n",
    "                date = pd.to_datetime(value[\"committer_date_readable\"]).dt.strftime(\"%B %d, %Y\").iloc[0]\n",
    "                date_prefix = \"From\"\n",
    "                value = int(value.iloc[0][key])\n",
    "            else: \n",
    "                value =  0\n",
    "            self.metrics[f\"linux/{arch}\"][key]= {\n",
    "                \"currentValue\": {\n",
    "                    \"value\":f\"{value} loc\", \n",
    "                    \"date\": f\"{date_prefix} {date}\"\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "linux_dfs = Linux()\n",
    "linux_dfs.fill_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_nonLinux(df, key, unit, prefix, apply_func=None):\n",
    "    date_prefix = \"\"\n",
    "    value = df.sort_values(by=\"committer_date_unix\").tail(1)\n",
    "    if not value.empty:\n",
    "        date = pd.to_datetime(value[\"committer_date_readable\"]).dt.strftime(\"%B %d, %Y\").iloc[0]\n",
    "        date_prefix = \"From\"\n",
    "        if apply_func:\n",
    "            value = apply_func(value.iloc[0][key])\n",
    "    else: \n",
    "        value =  0\n",
    "    return {\n",
    "        \"currentValue\": {\n",
    "        \"value\":f\"{prefix}{value} {unit}\", \n",
    "        \"date\": f\"{date_prefix} {date}\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_nonLinux(output_project_path, project, ignore_systems=[]):\n",
    "    df = read_dataframe(\"kconfig\", output_directory=output_project_path)\n",
    "    if ignore_systems != []:\n",
    "        df = df[~df[\"system\"].isin(ignore_systems)]\n",
    "    metrics = dict()\n",
    "    metrics[project] = dict()\n",
    "    metrics[project][\"source_lines_of_code\"]= get_latest_nonLinux(df=df, key=\"source_lines_of_code\", unit=\"loc\", prefix=\"\", apply_func=lambda v: int(v))\n",
    "    metrics[project][\"total-features\"]= get_latest_nonLinux(df=df, key=\"model-features\", unit=\"features\", prefix=\"\", apply_func=lambda v: int(v))\n",
    "    metrics[project][\"model-count-time\"]= get_latest_nonLinux(df=df, key=\"model-time\", unit=\"s\", prefix=\"\",apply_func=lambda v: int(v)// 1000000000)\n",
    "    metrics[project][\"model-count\"]= get_latest_nonLinux(df=df, key=\"model-literals\", unit=\"models\", prefix=\"\",apply_func=lambda v: int(v))\n",
    "    merge_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics_nonLinux(\"output-busybox\", project=\"busybox\", ignore_systems=[\"busybox-models\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
