{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# pip install plotly pandas statsmodels kaleido scipy nbformat jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from math import log10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIG VARIABLES\n",
    "init_json_path = \"vuetify-project/public/init.json\"\n",
    "linux_extractors = [\"KConfigReader\", \"KClause\"]\n",
    "output_base_path = \".\"\n",
    "\n",
    "nonlinux_projects = [\"busybox\"]\n",
    "ignore_systems = {\"busybox\": [\"busybox-models\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UTILITY FUNCTIONS\n",
    "\n",
    "def read_dataframe(\n",
    "    stage, dtype={}, usecols=None, file=None, output_directory=\"output-linux\"\n",
    "):\n",
    "    if not file:\n",
    "        file = \"output\"\n",
    "    df = pd.read_csv(\n",
    "        f\"{output_directory}/{stage}/{file}.csv\", dtype=dtype, usecols=usecols\n",
    "    )\n",
    "    if \"committer_date_unix\" in df:\n",
    "        df[\"committer_date\"] = df[\"committer_date_unix\"].apply(\n",
    "            lambda d: pd.to_datetime(d, unit=\"s\")\n",
    "        )\n",
    "    return df\n",
    "def replace_values(df):\n",
    "    df.replace(\"kconfigreader\", \"KConfigReader\", inplace=True)\n",
    "    df.replace(\"kmax\", \"KClause\", inplace=True)\n",
    "\n",
    "\n",
    "def big_log10(str):\n",
    "    return log10(int(str)) if not pd.isna(str) and str != \"\" else pd.NA\n",
    "\n",
    "\n",
    "def process_model_count(df_solve):\n",
    "    df_solve[\"model-count\"] = df_solve[\"model-count\"].replace(\"1\", \"\")\n",
    "    df_solve[\"model-count-log10\"] = (\n",
    "        df_solve[\"model-count\"].fillna(\"\").apply(big_log10).replace(0, np.nan)\n",
    "    )\n",
    "    df_solve[\"year\"] = df_solve[\"committer_date\"].apply(lambda d: int(d.year))\n",
    "\n",
    "\n",
    "def unify_solvers(df, columns=['model-count-unconstrained-log10']):\n",
    "    return df[['revision', 'committer_date', 'architecture', 'extractor', *columns]].drop_duplicates()\n",
    "\n",
    "def big_sum(series):\n",
    "    big_sum = sum([int(value) for value in series if not pd.isna(value) and value])\n",
    "    if big_sum > 0:\n",
    "        return len(str(big_sum))\n",
    "\n",
    "def write_object_to_file(obj, name):\n",
    "    with open(name, \"w\") as fp:\n",
    "        json.dump(obj, fp)\n",
    "\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path) as json_data:\n",
    "        return json.load(json_data)\n",
    "\n",
    "\n",
    "def merge_metrics(new):\n",
    "    old = read_json(init_json_path)\n",
    "\n",
    "    for proj, metrics in new.items():\n",
    "        for metric, values in metrics.items():\n",
    "            for name, value in values.items():\n",
    "                if proj not in old[\"projectData\"]:\n",
    "                    print(f\"{proj} not in init.json\")\n",
    "                    continue\n",
    "                old[\"projectData\"][proj][metric][name] = value\n",
    "    write_object_to_file(old, init_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linux:\n",
    "    def __init__(self, experiment_dir_name=None):\n",
    "        self.output_directory = os.path.join(output_base_path, experiment_dir_name or \"output-linux\")\n",
    "        self.df_kconfig = self.read_dataframe(\"kconfig\")\n",
    "        self.df_kconfig[\"year\"] = self.df_kconfig[\"committer_date\"].apply(\n",
    "            lambda d: int(d.year)\n",
    "        )\n",
    "        self.df_architectures = self.read_dataframe(\"read-linux-architectures\")\n",
    "        self.df_architectures = self.df_architectures.sort_values(by=\"committer_date\")\n",
    "        self.df_architectures[\"year\"] = self.df_architectures[\"committer_date\"].apply(\n",
    "            lambda d: int(d.year)\n",
    "        )\n",
    "        self.df_configs = self.read_dataframe(\"read-linux-configs\")\n",
    "        self.df_configs = self.df_configs[\n",
    "            ~self.df_configs[\"kconfig-file\"].str.contains(\"/um/\")\n",
    "        ]\n",
    "        self.df_config_types = self.read_dataframe(\n",
    "            \"read-linux-configs\", file=\"output.types\"\n",
    "        )\n",
    "        self.df_config_types = self.df_config_types[\n",
    "            ~self.df_config_types[\"kconfig-file\"].str.contains(\"/um/\")\n",
    "        ]\n",
    "        self.df_config_types = self.df_config_types.merge(\n",
    "            self.df_architectures[[\"revision\", \"committer_date\"]].drop_duplicates()\n",
    "        )\n",
    "        self.df_uvl = self.read_dataframe(\"model_to_uvl_featureide\")\n",
    "        self.df_smt = self.read_dataframe(\"model_to_smt_z3\")\n",
    "        self.df_dimacs = self.read_dataframe(\"dimacs\")\n",
    "        self.df_backbone_dimacs = self.read_dataframe(\"backbone-dimacs\")\n",
    "        self.df_solve = self.read_dataframe(\n",
    "            \"solve_model-count\", {\"model-count\": \"string\"}\n",
    "        )\n",
    "        process_model_count(self.df_solve)\n",
    "        if os.path.isfile(f'{self.output_directory}/model-count-with-6h-timeout.csv'):\n",
    "            self.df_solve_6h = pd.read_csv(f'{self.output_directory}/model-count-with-6h-timeout.csv', dtype={'model-count': 'string'})\n",
    "            self.df_solve_6h = self.df_backbone_dimacs.merge(self.df_solve_6h)\n",
    "            process_model_count(self.df_solve_6h)\n",
    "            self.df_solve = pd.merge(self.df_solve, self.df_solve_6h[['revision','architecture', 'extractor', 'backbone.dimacs-analyzer']], indicator=True, how='outer') \\\n",
    "                .query('_merge==\"left_only\"') \\\n",
    "                .drop('_merge', axis=1)\n",
    "            self.df_solve = pd.concat([self.df_solve, self.df_solve_6h])\n",
    "        else:\n",
    "            self.df_solve_6h = None\n",
    "        for df in [\n",
    "            self.df_kconfig,\n",
    "            self.df_uvl,\n",
    "            self.df_smt,\n",
    "            self.df_dimacs,\n",
    "            self.df_backbone_dimacs,\n",
    "            self.df_solve,\n",
    "        ]:\n",
    "            df.replace(\"kconfigreader\", \"KConfigReader\", inplace=True)\n",
    "            df.replace(\"kmax\", \"KClause\", inplace=True)\n",
    "        self.df_configs_configurable = self.df_configs.copy()\n",
    "        self.df_configs_configurable[\"configurable\"] = False\n",
    "        with open(f\"{self.output_directory}/linux-features.dat\", \"rb\") as f:\n",
    "            [\n",
    "                self.features_by_kind_per_architecture,\n",
    "                self.df_extractor_comparison,\n",
    "                self.potential_misses_grep,\n",
    "                self.potential_misses_kmax,\n",
    "                self.df_configs_configurable,\n",
    "            ] = pickle.load(f)\n",
    "\n",
    "        replace_values(self.features_by_kind_per_architecture)\n",
    "        self.df_features = pd.merge(\n",
    "            self.df_architectures, self.features_by_kind_per_architecture, how=\"outer\"\n",
    "        ).sort_values(by=\"committer_date\")\n",
    "        self.df_features = pd.merge(\n",
    "            self.df_kconfig, self.df_features, how=\"outer\"\n",
    "        ).sort_values(by=\"committer_date\")\n",
    "        self.df_total_features = (\n",
    "            self.df_features.groupby([\"extractor\", \"revision\"])\n",
    "            .agg({\"#total_features\": \"min\"})\n",
    "            .reset_index()\n",
    "        )\n",
    "        self.df_total_features = pd.merge(\n",
    "            self.df_kconfig[[\"committer_date\", \"revision\"]].drop_duplicates(),\n",
    "            self.df_total_features,\n",
    "        )\n",
    "        df_solve_unconstrained = self.df_solve.merge(self.df_features)\n",
    "        df_solve_unconstrained[\"model-count-unconstrained\"] = df_solve_unconstrained.apply(\n",
    "            lambda row: str(\n",
    "                int(row[\"model-count\"])\n",
    "                * (2 ** int(row[\"unconstrained_bools\"]))\n",
    "                * (3 ** int(row[\"unconstrained_tristates\"]))\n",
    "            )\n",
    "            if not pd.isna(row[\"model-count\"]) and row[\"model-count\"] != \"\"\n",
    "            else pd.NA,\n",
    "            axis=1,\n",
    "        )\n",
    "        df_solve_unconstrained[\"model-count-unconstrained-log10\"] = (\n",
    "            df_solve_unconstrained[\"model-count-unconstrained\"]\n",
    "            .fillna(\"\")\n",
    "            .map(big_log10)\n",
    "            .replace(0, np.nan)\n",
    "        )\n",
    "        df_solve_unconstrained[\"similarity\"] = df_solve_unconstrained.apply(\n",
    "            lambda row: int(row[\"model-count\"]) / int(row[\"model-count-unconstrained\"])\n",
    "            if not pd.isna(row[\"model-count\"]) and row[\"model-count\"] != \"\"\n",
    "            else pd.NA,\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "\n",
    "        def unify_solvers(df, columns=['model-count-unconstrained-log10']):\n",
    "            return df[['revision', 'committer_date', 'architecture', 'extractor', *columns]].drop_duplicates()\n",
    "\n",
    "        def big_sum(series):\n",
    "            big_sum = sum([int(value) for value in series if not pd.isna(value) and value])\n",
    "            if big_sum > 0:\n",
    "                return len(str(big_sum))\n",
    "            \n",
    "        self.df_solve_slice = df_solve_unconstrained[df_solve_unconstrained['year'] <= 2013]\n",
    "        self.df_solve_failures = self.df_solve_slice.groupby(['extractor', 'revision', 'architecture'], dropna=False).agg({'model-count-unconstrained-log10': lambda x: (True in list(pd.notna(x)) or pd.NA)}).reset_index()\n",
    "        self.df_solve_group = self.df_solve_failures.groupby(['extractor', 'revision'], dropna=False)\n",
    "        self.df_solve_failures = (self.df_solve_group['model-count-unconstrained-log10'].size() - self.df_solve_group['model-count-unconstrained-log10'].count()).reset_index()\n",
    "        self.df_solve_failures['is-upper-bound'] = self.df_solve_failures['model-count-unconstrained-log10'] == 0\n",
    "        self.df_solve_failures = self.df_solve_failures.rename(columns={'model-count-unconstrained-log10': 'failures'})\n",
    "        self.df_solve_total = unify_solvers(pd.merge(self.df_solve_slice, self.df_solve_failures), ['model-count-unconstrained', 'model-count-unconstrained-log10', 'is-upper-bound', 'failures', 'year'])\n",
    "        self.df_solve_total = self.df_solve_total.groupby(['extractor', 'committer_date', 'year']).agg({'model-count-unconstrained': big_sum, 'is-upper-bound': 'min', 'failures': 'min'}).reset_index()\n",
    "    \n",
    "        self.keymap = {\n",
    "            \"#total_features\": \"total-features\", \n",
    "            \"#features\": \"features\", \n",
    "            \"backbone.dimacs-analyzer-time\": \"model-count-time\",\n",
    "            \"model-count-unconstrained-log10\": \"model-count\",\n",
    "            \"model-count-unconstrained\": \"model-count\",\n",
    "            \"source_lines_of_code\": \"source_lines_of_code\"\n",
    "        }\n",
    "        self.metrics = {f\"linux/{arch}\": dict() for arch in self.df_kconfig[\"architecture\"].unique()}\n",
    "        self.metrics[\"linux/all\"] = dict()\n",
    "\n",
    "    def read_dataframe(self, stage, dtype={}, usecols=None, file=None):\n",
    "        if not file:\n",
    "            file = \"output\"\n",
    "        df = pd.read_csv(\n",
    "            f\"{self.output_directory}/{stage}/{file}.csv\", dtype=dtype, usecols=usecols\n",
    "        )\n",
    "        if \"committer_date_unix\" in df:\n",
    "            df[\"committer_date\"] = df[\"committer_date_unix\"].apply(\n",
    "                lambda d: pd.to_datetime(d, unit=\"s\")\n",
    "            )\n",
    "        return df\n",
    "    def solver_successes(self, solver):\n",
    "        df_solve_for_solver = self.df_solve_attempts[~self.df_solve_attempts['model-count'].isna()]\n",
    "        df_solve_for_solver = df_solve_for_solver[df_solve_for_solver['backbone.dimacs-analyzer'] == solver]\n",
    "        return set(df_solve_for_solver['extractor'] + ',' + df_solve_for_solver['revision'] + ',' + df_solve_for_solver['architecture'])\n",
    "\n",
    "    def filter_for_architecture(self, df, arch):\n",
    "        return df[df[\"architecture\"] == arch]\n",
    "\n",
    "    def differentiate_extractors(self,arch, df, sortBy, key, prefix, unit, apply_func=None):\n",
    "        \"\"\"returns: {\n",
    "            \"extractor1\": {\n",
    "                \"value\": \"<prefix><value1> <unit>\",\n",
    "                \"date\": \"From <date1>\"},\n",
    "            \"extractor2\": {\n",
    "                \"value\": \"<prefix><value2> <unit>\",\n",
    "                \"date\": \"From <date2>\"\n",
    "            }\n",
    "        \"\"\"\n",
    "        vals = {\"currentValue\": dict()}\n",
    "        for extractor in df[\"extractor\"].unique():\n",
    "            history = {\"history\": dict()}\n",
    "            df_ex = df[df[\"extractor\"] == extractor]\n",
    "            if df_ex.empty:\n",
    "                print(key,df[\"extractor\"].unique(), extractor )\n",
    "            ex_value = df_ex.dropna().sort_values(sortBy).last_valid_index()\n",
    "            if ex_value not in df_ex.index:\n",
    "                vals[\"currentValue\"][extractor] = {\n",
    "                    \"currentValue\": {                \n",
    "                        \"value\": f\"0 {unit}\",\n",
    "                        \"date\": f\"Date not Found\"\n",
    "                    },\n",
    "                    \"history\": history[\"history\"]\n",
    "                }\n",
    "                continue\n",
    "            ex_value = df_ex.loc[ex_value]\n",
    "            date = \"Date not Found\"\n",
    "            date_prefix = \"\"\n",
    "            if not ex_value.empty:\n",
    "                date = ex_value[\"committer_date\"]\n",
    "                history = self.try_history(\n",
    "                    arch=arch,\n",
    "                    df=df_ex,\n",
    "                    last_date=date,\n",
    "                    key=key,\n",
    "                    unit=unit,\n",
    "                    prefix=prefix,\n",
    "                    apply_func=apply_func, \n",
    "                    last_value=ex_value[key],\n",
    "                    date_func=lambda ym: df_ex[\"committer_date\"].dt.strftime(\"%Y\") == ym.strftime(\"%Y\")\n",
    "                )\n",
    "                date = date.strftime(\"%B %d, %Y\")\n",
    "                date_prefix = \"From\"\n",
    "                if apply_func:\n",
    "                    ex_value = apply_func(ex_value[key])\n",
    "            else:\n",
    "                ex_value = 0\n",
    "                prefix = \"\"\n",
    "                \n",
    "            vals[\"currentValue\"][extractor] = {\n",
    "                \"currentValue\": {                \n",
    "                    \"value\": f\"{prefix}{ex_value} {unit}\",\n",
    "                    \"date\": f\"{date_prefix} {date}\"\n",
    "                },\n",
    "                \"history\": history\n",
    "            }\n",
    "        return vals\n",
    "\n",
    "    def model_count_time_latest(self):\n",
    "        archs = list(self.df_kconfig[\"architecture\"].unique())\n",
    "        for arch in archs:\n",
    "            df_arch = self.df_solve_slice[self.df_solve_slice[\"architecture\"]==arch]\n",
    "            key = \"backbone.dimacs-analyzer-time\"\n",
    "            extractor_values = self.differentiate_extractors(arch=arch,df=df_arch, sortBy=\"committer_date_unix\", key=key, prefix=\"10^\", unit=\"s\", apply_func=lambda v: int(v)//1000000000)\n",
    "            self.metrics[f\"linux/{arch}\"][\"model-count-time\"]= extractor_values\n",
    "\n",
    "    def model_count_latest(self):\n",
    "        archs = list(self.df_kconfig[\"architecture\"].unique())\n",
    "        archs.append(\"all\")\n",
    "        for arch in archs:\n",
    "            df_arch = self.df_solve_slice[self.df_solve_slice[\"architecture\"]==arch]\n",
    "            key = \"model-count-unconstrained-log10\"\n",
    "            sortBy = \"committer_date_unix\"\n",
    "            if arch == 'all':\n",
    "                df_arch = self.df_solve_total\n",
    "                key = \"model-count-unconstrained\"\n",
    "                sortBy = \"committer_date\"\n",
    "            extractor_values = self.differentiate_extractors(arch=arch,df=df_arch, sortBy=sortBy, key=key, prefix=\"10^\", unit=\"models\", apply_func=lambda v: int(v))\n",
    "            self.metrics[f\"linux/{arch}\"][\"model-count\"]= extractor_values\n",
    "        \n",
    "    def fill_metrics(self):\n",
    "        self.total_features_latest()\n",
    "        self.features_latest()\n",
    "        self.sloc_latest()\n",
    "        self.model_count_latest()\n",
    "        self.model_count_time_latest()\n",
    "        merge_metrics(self.metrics)\n",
    "\n",
    "    def total_features_latest(self):\n",
    "        extractor_values = self.differentiate_extractors(arch=\"all\", df=self.df_total_features, sortBy=\"committer_date\", key=\"#total_features\", prefix=\"\", unit=\"features\", apply_func=lambda v: int(v))\n",
    "        self.metrics[\"linux/all\"][\"total-features\"] = extractor_values\n",
    "\n",
    "    def features_latest(self):\n",
    "        archs = list(self.df_kconfig[\"architecture\"].unique())\n",
    "        for architecture in archs:\n",
    "            df = self.filter_for_architecture(self.df_features, architecture)\n",
    "            extractor_values = self.differentiate_extractors(arch=architecture, df=df, sortBy=\"committer_date\", key=\"#features\", prefix=\"\", unit=\"features\", apply_func=lambda v: int(v))\n",
    "            self.metrics[f\"linux/{architecture}\"][\"features\"] = extractor_values\n",
    "\n",
    "    def sloc_latest(self):\n",
    "        archs = list(self.df_kconfig[\"architecture\"].unique())\n",
    "        archs.append(\"all\")\n",
    "        key = \"source_lines_of_code\"\n",
    "        for arch in archs:\n",
    "            history = {\"history\": dict()}\n",
    "            date = \"Date not Found\"\n",
    "            date_prefix = \"\"\n",
    "            df_arch = self.filter_for_architecture(self.df_kconfig, arch)\n",
    "            if arch == \"all\":\n",
    "                df_arch = self.df_kconfig\n",
    "            value = df_arch.sort_values(by=\"committer_date_unix\").tail(1)\n",
    "            if not value.empty:\n",
    "                date = pd.to_datetime(value[\"committer_date_readable\"]).iloc[0]\n",
    "                history = self.try_history(\n",
    "                    arch=arch,\n",
    "                    df=df_arch,\n",
    "                    last_date=date,\n",
    "                    key=key,\n",
    "                    unit=\"loc\",\n",
    "                    prefix=\"\",\n",
    "                    apply_func=lambda x: int(x),\n",
    "                    last_value=value.iloc[0][key],\n",
    "                    date_func=lambda ym: df_arch[\"committer_date_readable\"].str.contains(ym.strftime(\"%Y-%m\"))\n",
    "                )\n",
    "                date = date.strftime(\"%B %d, %Y\")\n",
    "                date_prefix = \"From\"\n",
    "                value = int(value.iloc[0][key])\n",
    "            else: \n",
    "                value =  0\n",
    "            self.metrics[f\"linux/{arch}\"][key]= {\n",
    "                \"currentValue\": {\n",
    "                    \"value\":f\"{value} loc\", \n",
    "                    \"date\": f\"{date_prefix} {date}\"\n",
    "                    },\n",
    "                \"history\": history\n",
    "            }\n",
    "    def try_history(self, arch, df, last_date, key, unit, prefix, last_value, date_func, apply_func=None, ):\n",
    "        history = [1, 2, 5, 10]\n",
    "        if key not in self.metrics[f\"linux/{arch}\"]:\n",
    "            self.metrics[f\"linux/{arch}\"][self.keymap[key]] = {}\n",
    "        self.metrics[f\"linux/{arch}\"][self.keymap[key]][\"history\"] = dict() \n",
    "        history_vals = dict()\n",
    "        for last in history:\n",
    "            ym = last_date - pd.Timedelta(last*365, \"d\")\n",
    "            value = df[date_func(ym)].tail(1)\n",
    "            if value.empty:\n",
    "                ym = ym - pd.Timedelta(30, \"d\")\n",
    "                value = df[date_func(ym)].tail(1)\n",
    "            if value.empty:\n",
    "                ym = ym + pd.Timedelta(30, \"d\")\n",
    "                value = df[date_func(ym)].tail(1)\n",
    "            if value.empty:\n",
    "                value = df[date_func(ym)].tail(1)\n",
    "            if value.empty:\n",
    "                continue            \n",
    "            value = value.iloc[0].replace(pd.NA, np.nan)[key]\n",
    "            if np.isnan(value):\n",
    "                continue\n",
    "            if apply_func:\n",
    "                last_value = apply_func(last_value)\n",
    "                value = apply_func(value)\n",
    "            if value != 0:\n",
    "                diff = round(100*(value-last_value)/value,1)\n",
    "                history_vals[f\"{last}-years-before\"] = {\n",
    "                    \"value\": f\"{prefix}{value} {unit} ({diff:+}%)\", \n",
    "                    \"date\": ym.strftime(\"%B %d, %Y\")\n",
    "                } \n",
    "        return history_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinux:\n",
    "    def __init__(self, project, ignore_systems=[]):\n",
    "        self.output_directory = os.path.join(output_base_path, f\"output-{project}\")\n",
    "        self.project = project\n",
    "        self.df = read_dataframe(\"kconfig\", output_directory=self.output_directory)\n",
    "        if ignore_systems != []:\n",
    "            self.df = self.df[~self.df[\"system\"].isin(ignore_systems)]\n",
    "        self.keymap = {\n",
    "            \"source_lines_of_code\": \"source_lines_of_code\", \n",
    "            \"model-features\": \"total-features\", \n",
    "            \"model-time\": \"model-count-time\", \n",
    "            \"model-literals\": \"model-count\"\n",
    "        }\n",
    "        self.metrics = dict()\n",
    "        self.metrics[project] = dict()\n",
    "    \n",
    "    def try_history(self, last_date, key, unit, prefix, last_value, apply_func=None, ):\n",
    "        history = [1, 2, 5, 10]\n",
    "        curyear = last_date.iloc[0]\n",
    "        if self.keymap[key] not in self.metrics[self.project]:\n",
    "            self.metrics[self.project][self.keymap[key]] = {}\n",
    "        self.metrics[self.project][self.keymap[key]][\"history\"] = dict() \n",
    "        history_vals = {\"history\": dict()}\n",
    "        for last in history:\n",
    "            ym = curyear - pd.Timedelta(last*365, \"d\")\n",
    "            value = self.df[self.df[\"committer_date_readable\"].str.contains(ym.strftime(\"%Y-%m\"))].tail(1)\n",
    "            if value.empty:\n",
    "                ym = ym - pd.Timedelta(30, \"d\")\n",
    "                value = self.df[self.df[\"committer_date_readable\"].str.contains(ym.strftime(\"%Y-%m\"))].tail(1)\n",
    "            if value.empty:\n",
    "                ym = ym + pd.Timedelta(30, \"d\")\n",
    "                value = self.df[self.df[\"committer_date_readable\"].str.contains(ym.strftime(\"%Y-%m\"))].tail(1)\n",
    "            if value.empty:\n",
    "                value = self.df[self.df[\"committer_date_readable\"].str.contains(ym.strftime(\"%Y\"))].tail(1)\n",
    "            if value.empty:\n",
    "                continue            \n",
    "            value = value.iloc[0][key]\n",
    "            if apply_func:\n",
    "                last_value = apply_func(last_value)\n",
    "                value = apply_func(value)\n",
    "            diff = round(100*(value-last_value)/value,1)\n",
    "            \n",
    "            history_vals[\"history\"][f\"{last}-years-before\"] = {\n",
    "                \"value\": f\"{prefix}{value} {unit} ({diff:+}%)\", \n",
    "                \"date\": ym.strftime(\"%B %d, %Y\")\n",
    "            } \n",
    "        return history_vals\n",
    "\n",
    "    def get_latest_nonLinux(self, key, unit, prefix, apply_func=None):\n",
    "        date_prefix = \"\"\n",
    "        value = self.df.sort_values(by=\"committer_date_unix\").tail(1)\n",
    "        history = {\"history\": dict()}\n",
    "        if not value.empty:\n",
    "            date = pd.to_datetime(value[\"committer_date_readable\"])\n",
    "            history = self.try_history(last_date=date, key=key, unit=unit, prefix=prefix, apply_func=apply_func, last_value=value.iloc[0][key])\n",
    "            date = date.dt.strftime(\"%B %d, %Y\").iloc[0]\n",
    "            date_prefix = \"From\"\n",
    "            value = value.iloc[0][key]\n",
    "            if apply_func:\n",
    "                value = apply_func(value)\n",
    "        else: \n",
    "            value =  0\n",
    "        self.metrics[self.project][self.keymap[key]] = {\n",
    "            \"currentValue\": {\n",
    "                \"value\":f\"{prefix}{value} {unit}\", \n",
    "                \"date\": f\"{date_prefix} {date}\"\n",
    "            },\n",
    "            \"history\": history[\"history\"]\n",
    "        }\n",
    "    def fill_metrics(self):\n",
    "        self.get_latest_nonLinux(key=\"source_lines_of_code\", unit=\"loc\", prefix=\"\", apply_func=lambda v: int(v))\n",
    "        self.get_latest_nonLinux(key=\"model-features\", unit=\"features\", prefix=\"\", apply_func=lambda v: int(v))\n",
    "        self.get_latest_nonLinux(key=\"model-time\", unit=\"s\", prefix=\"\",apply_func=lambda v: round(v/ 1000000000, 3))\n",
    "        self.get_latest_nonLinux(key=\"model-literals\", unit=\"models\", prefix=\"\",apply_func=lambda v: int(v))\n",
    "        merge_metrics(self.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nonlinux in nonlinux_projects:\n",
    "nl = NonLinux(\"busybox\", ignore_systems=ignore_systems[\"busybox\"])\n",
    "nl.fill_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "linux_dfs = Linux()\n",
    "linux_dfs.fill_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
